{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjJE3tTz_XWx"
   },
   "source": [
    "# ĐỒ ÁN CUỐI KỲ\n",
    "## Nhóm 07\n",
    "\n",
    "### Thành viên\n",
    "1. Lưu Hữu Tình - 21424056\n",
    "2. Đặng Hồ Hoàng Duy - 21424073\n",
    "3. Huỳnh Văn Thái - 21424088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IA1jYFw1Ov3"
   },
   "source": [
    "<h1>\n",
    "<font color=\"blue\">Chủ đề: THU THẬP DỮ LIỆU THÔNG TIN CÁC ĐỘI BÓNG</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xaNockS1Ov3"
   },
   "source": [
    "# PHẦN 1: QUY TRÌNH KHOA HỌC DỮ LIỆU\n",
    "\n",
    "## A. Thu thập dữ liệu (Data collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbEN_2ZCvmSW",
    "outputId": "549223cd-530b-4b34-9c83-6a576e58eb79"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Run with google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEkOb7yNBWVz"
   },
   "source": [
    "<b>Import</b>\n",
    "\n",
    "Cài đặt các thư viện, gói cho việc collect data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7ZL9mrLkc3Dp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in d:\\users\\asus\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: lxml>=4.3.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (4.5.2)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (1.0.6)\n",
      "Requirement already satisfied: tldextract in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (3.4.0)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (5.5.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\" in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (0.7.0)\n",
      "Requirement already satisfied: setuptools in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: parsel>=1.5.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: packaging in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (20.4)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (2.1.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (23.0.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: cryptography>=3.4.6 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (39.0.2)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from scrapy) (22.10.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.0.12)\n",
      "Requirement already satisfied: requests>=2.1.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.24.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: idna in d:\\users\\asus\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.10)\n",
      "Requirement already satisfied: six in d:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging->scrapy) (2.4.7)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (22.2.0)\n",
      "Requirement already satisfied: pyasn1 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in d:\\users\\asus\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from cryptography>=3.4.6->scrapy) (1.14.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2; platform_system == \"Windows\" in d:\\users\\asus\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.25.9)\n",
      "Requirement already satisfied: pycparser in d:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.20)\n",
      "Requirement already satisfied: spider3 in d:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "!pip install spider3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Nah1kqOV74Wk"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from pandas.testing import assert_frame_equal\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9hY-O1W_omi"
   },
   "source": [
    "Bóng đá là môn thể thao hấp dẫn nhất hành tinh. Chúng ta sẽ khởi động môn học bằng việc thu thập dữ liệu của các CLB bóng đá. SoFIFA (https://sofifa.com/) là một trang web lưu trữ dữ liệu của các CLB trong trò chơi bóng đá nổi tiếng FIFA 23 mà có các chỉ số phản ánh gần đúng với phong độ của các CLB bóng đá ngoài đời. Trong phần này, nhiệm vụ đầu tiên là cần thu thập ID của các CLB.\n",
    "\n",
    "#### Tạo một project mới với scrapy\n",
    "\n",
    "Để sử dụng thư viện scrapy sau khi cài đặt xong, gọi câu lệnh như bên dưới để bắt đầu tạo một project mới với scrapy với tên gọi là `fifa_crawler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ndHFd1lWnUj",
    "outputId": "2f56699c-4bd8-4e5d-d10c-f1143332a399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Module 'fifa_crawler' already exists\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject fifa_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyLNgVqWvt3Z",
    "outputId": "1e4ecf29-0e31-4d21-a6aa-701d556e0e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'fifa_crawler/fifa_crawler'\n",
      "C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\n"
     ]
    }
   ],
   "source": [
    "cd fifa_crawler/fifa_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnysguzmAOMV"
   },
   "source": [
    "Sau khi tạo xong project với scrapy, vì thu thập dữ liệu với thư viện này không cho phép xài notebook trực tiếp nên các bạn sau khi hoàn thành xong class `collect_team_url` (scrapy.Spider) ở bên dưới, tạo một file có tên `collect_teams_urls.py` vào đường dẫn sau `fifa_crawler/fifa_crawler/spiders/collect_teams_urls.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr3zUqrt1Ov8"
   },
   "source": [
    "<b>Tạo file collect_teams_urls.py</b>\n",
    "\n",
    "1. File sẽ collect data từ trang https://sofifa.com/teams?col=oa&sort=desc&offset=0\n",
    "2. Cấu trúc trả về là json, danh sách các url của mỗi đội bóng\n",
    "```json\n",
    "[\n",
    "{\"team_url\": \"/team/10\"},\n",
    "{\"team_url\": \"/team/21\"},\n",
    "...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nsIYPdgWeJ0z"
   },
   "outputs": [],
   "source": [
    "# File: \\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_urls.py\n",
    "import scrapy\n",
    "\n",
    "class collect_team_url(scrapy.Spider):\n",
    "  name='teams_urls' \n",
    "  \n",
    "  def start_requests(self):\n",
    "    urls = ['https://sofifa.com/teams?col=oa&sort=desc&offset=0']\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    for i in range(1,13):\n",
    "        # Mỗi trang hiển thị 60 cầu thủ\n",
    "        url = f'https://sofifa.com/teams?col=oa&sort=desc&offset={i*60}' \n",
    "        urls.append(url)\n",
    "\n",
    "    for url in urls:\n",
    "      yield scrapy.Request(url=url, headers=headers, callback=self.parse)\n",
    "\n",
    "  def parse(self, response):\n",
    "    # <td class=\"col-name-wide\"> \n",
    "    #  <a href=\"/team/10/manchester-city/\">\n",
    "    team_urls = response.css('td.col-name-wide a::attr(href)').extract()\n",
    "    team_ids = []\n",
    "    for url in team_urls:\n",
    "      # Get team /team/10\n",
    "      if url.split('/')[1] == \"team\":\n",
    "        team_id = url.split('/')[2]\n",
    "        team_url = f\"/team/{team_id}\"\n",
    "        team_item = {\"team_url\": team_url}\n",
    "        yield team_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svTVfwL51Ov9"
   },
   "source": [
    "<b>Start collect data từ file collect_teams_urls.py</b>\n",
    "\n",
    "Data collect được lưu vào dataset/teams_urls.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kLoJ44s6e9NG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "2023-05-07 14:40:52 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: fifa_crawler)\n",
      "2023-05-07 14:40:52 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform Windows-10-10.0.22000-SP0\n",
      "2023-05-07 14:40:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'fifa_crawler',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'fifa_crawler.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['fifa_crawler.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-05-07 14:40:52 [asyncio] DEBUG: Using selector: SelectSelector\n",
      "2023-05-07 14:40:52 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-05-07 14:40:52 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
      "2023-05-07 14:40:52 [scrapy.extensions.telnet] INFO: Telnet Password: 51e3b061ba56c15c\n",
      "2023-05-07 14:40:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-05-07 14:40:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-05-07 14:40:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-05-07 14:40:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-05-07 14:40:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-05-07 14:40:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-05-07 14:40:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://sofifa.com/robots.txt> (referer: None)\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=0> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=60> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=420> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=180> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=240> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=300> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=120> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/10'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/21'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/243'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/9'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1318'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/73'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/5'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1335'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1337'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1354'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1362'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1369'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/240'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/241'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/11'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/44'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/45'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1343'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/105035'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/18'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/22'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/112172'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/47'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/13'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1325'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/48'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/52'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1370'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/481'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/483'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/2'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/32'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/46'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1328'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/448'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/449'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/457'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/234'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/19'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1824'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/110374'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/39'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1331'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/95'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/110'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/219'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/236'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/237'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/111111'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/14'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1808'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/23'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1322'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/10029'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/66'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1860'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/69'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/325'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=0>\n",
      "\n",
      "{'team_url': '/team/1353'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/74'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1386'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/450'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/452'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/245'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/7'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1799'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/8'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1043'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/25'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1330'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/64'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/65'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/72'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1352'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1363'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1876'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1896'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/383'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1925'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/144'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/175'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/436'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/111811'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/461'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/479'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/480'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/110062'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/247'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1035'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/17'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1831'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/55'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/326'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/71'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/327'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1359'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1877'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1366'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/111462'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/111974'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1387'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/169'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1968'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/189'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/453'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/462'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/231'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/246'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/266'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1039'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/278'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1048'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/31'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/36'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/110373'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/54'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/100409'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/1861'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=60>\n",
      "\n",
      "{'team_url': '/team/70'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/322'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1819'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/97'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111722'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/76'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/77'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/81'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/91'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/94'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112224'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112494'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/110968'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1915'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/897'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/898'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/113796'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1926'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/110982'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/647'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/113044'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/110998'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112791'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/111768'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/110500'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/433'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/114611'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1474'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1478'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/199'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/711'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/28'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/112670'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110636'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1334'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/57'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111674'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110395'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111434'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1356'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1874'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110935'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/607'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1888'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111715'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/109'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1903'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/900'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110980'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1415'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/101016'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/101059'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111052'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/101083'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110556'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111325'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/2014'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/741'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111339'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/101100'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/101101'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/101105'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/113142'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1530'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/3'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/112134'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/263'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1800'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1801'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110093'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111117'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/614'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/873'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1898'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110975'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112001'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112513'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1923'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110981'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110986'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/101007'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/918'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1952'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111008'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111010'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110502'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/101033'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112809'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111019'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/688'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111539'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1473'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112578'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/744'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110827'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/101103'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/101104'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/100851'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/244'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/101108'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111724'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1909'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1910'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1913'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/634'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1919'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/896'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111235'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/110724'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/110984'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/110738'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/101020'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/112540'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/670'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1438'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/159'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111013'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111014'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1960'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/171'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/687'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/101041'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/691'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/696'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/697'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1477'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111817'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/459'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/205'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/114640'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/209'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1745'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/78'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/112472'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1906'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1943'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/166'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1746'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/468'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/101085'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1796'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/267'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/112139'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1041'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1053'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/38'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1853'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/86'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/598'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1367'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/347'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1884'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/605'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1886'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/111455'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/378'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/379'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/110468'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/393'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/110741'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/160'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/673'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/206'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/472'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/217'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/230'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/110832'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1792'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1794'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1795'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/12'}\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=480> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/456'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112585'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1738'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1744'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/983'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/111065'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/101084'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/2013'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/224'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/111328'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112096'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/111329'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/2023'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/101109'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/110069'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1786'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1788'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/523'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1805'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/270'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112914'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/100628'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1814'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/1816'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112408'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/33'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/111400'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/298'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/299'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/112427'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/300'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/110902'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/573'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=420>\n",
      "\n",
      "{'team_url': '/team/113981'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1806'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/15'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1815'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111651'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/10020'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111140'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111144'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111657'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/10031'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/50'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1842'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110396'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1341'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/110406'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/114510'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/80'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111710'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/1887'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=180>\n",
      "\n",
      "{'team_url': '/team/111711'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/101110'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/252'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1793'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110081'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/260'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/518'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/269'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/29'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/100135'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1832'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/10030'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1843'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1847'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1848'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111928'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/320'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/576'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/324'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112965'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112716'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110930'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111701'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/88'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/10846'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/110178'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1892'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1750'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/112606'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/226'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/485'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111334'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/232'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1516'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/114161'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/114162'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/110580'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/1013'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/112885'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/100087'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/110839'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/110329'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/112893'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/113149'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/112390'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/271'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/100888'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/27'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/111138'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/100646'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/112689'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/570'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1809'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/34'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/294'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1837'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/819'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/567'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1355'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/112996'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/1908'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/101014'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/674'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/101047'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/191'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/463'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/211'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/467'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/101088'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/229'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/517'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=120>\n",
      "\n",
      "{'team_url': '/team/112908'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/111716'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/1893'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/112744'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=240>\n",
      "\n",
      "{'team_url': '/team/106'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/59'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=300>\n",
      "\n",
      "{'team_url': '/team/68'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/62'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/319'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/113217'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/112707'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/116295'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/89'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/114023'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/1905'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/111473'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110456'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110969'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/114554'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110978'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/113029'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/111239'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/1929'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110989'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110990'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110993'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/1938'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/114582'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/920'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110745'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/113057'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110765'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/112558'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/1971'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110776'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/700'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/190'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/702'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/708'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110532'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/111822'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/113378'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110569'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/111086'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110831'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/111088'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/503'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/112120'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110075'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/4'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/115716'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/2055'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/2056'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/780'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/272'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/1813'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/113182'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/115494'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/116007'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/111659'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110645'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/112695'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/58'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110394'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/580'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110955'}\n",
      "2023-05-07 14:40:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=480>\n",
      "\n",
      "{'team_url': '/team/110967'}\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=540> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=720> (referer: None)\n",
      "2023-05-07 14:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=600> (referer: None)\n",
      "2023-05-07 14:40:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=660> (referer: None)\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111993'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111766'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110970'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/127'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112511'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/645'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/116360'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1932'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1933'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/114580'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110747'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111004'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110749'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110750'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1951'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110751'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/113058'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1443'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/420'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112552'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/114600'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/181'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112572'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/15040'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/2017'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/114147'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111332'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/487'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/101097'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/746'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/492'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110321'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111091'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112115'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111092'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/506'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1787'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1790'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110078'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110592'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110597'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1798'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1944'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111769'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1947'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/115358'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/15009'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111779'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1446'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1713'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113345'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/100804'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/100805'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1755'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/100325'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111083'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113387'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/112883'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110581'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113926'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111395'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/112163'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111396'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111398'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111399'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/561'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/306'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113458'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110676'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/112985'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110691'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/357'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/361'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110194'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1928'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1930'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/149'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/100759'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/15001'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/922'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/111774'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1439'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110752'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/418'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110501'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/781'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/526'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1569'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1825'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111393'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/301'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112199'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/82'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/83'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/114004'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/111707'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112990'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/621'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/631'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112505'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/112260'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/143'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/1936'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=540>\n",
      "\n",
      "{'team_url': '/team/110482'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1958'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/15015'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1962'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/114604'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1463'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1756'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113380'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113892'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110313'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113146'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/110587'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/112126'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1802'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1803'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/1804'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=600>\n",
      "\n",
      "{'team_url': '/team/113459'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113743'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/381'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1931'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1934'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1940'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113300'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/111773'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1456'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1480'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112072'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1757'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/110890'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1596'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/92'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112222'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/121'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112254'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112259'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/142'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1935'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113299'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113301'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113302'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/423'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/432'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/445'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/114628'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/15048'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/110799'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/981'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112378'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/115489'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/305'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/837'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113298'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1941'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1955'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/563'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113297'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/111629'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/834'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113257'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112541'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/111839'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/114168'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/111131'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/1572'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/113040'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112429'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/111132'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=660>\n",
      "\n",
      "{'team_url': '/team/112978'}\n",
      "2023-05-07 14:40:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/teams?col=oa&sort=desc&offset=360> (referer: None)\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110404'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112713'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1871'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110929'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111706'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111708'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1917'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/894'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1413'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/116361'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110987'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1939'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/114326'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/100761'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110746'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112026'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/15005'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/417'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111011'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/101028'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/165'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111527'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/680'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/681'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1961'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/682'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/689'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110770'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/693'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/694'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/695'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/698'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112828'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111042'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/710'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/203'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/210'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/982'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111326'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110816'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/100081'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/101112'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/110588'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/254'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/256'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1797'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112392'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112393'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1807'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/100632'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/543'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111139'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/111397'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112168'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/10032'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/308'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/820'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/310'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/112184'}\n",
      "2023-05-07 14:40:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/teams?col=oa&sort=desc&offset=360>\n",
      "\n",
      "{'team_url': '/team/1854'}\n",
      "2023-05-07 14:40:54 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-05-07 14:40:54 [scrapy.extensions.feedexport] INFO: Stored json feed (711 items) in: dataset/teams_urls.json\n",
      "2023-05-07 14:40:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 4472,\n",
      " 'downloader/request_count': 14,\n",
      " 'downloader/request_method_count/GET': 14,\n",
      " 'downloader/response_bytes': 177516,\n",
      " 'downloader/response_count': 14,\n",
      " 'downloader/response_status_count/200': 13,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 1.941024,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 5, 7, 7, 40, 54, 837099),\n",
      " 'httpcompression/response_bytes': 1503296,\n",
      " 'httpcompression/response_count': 14,\n",
      " 'item_scraped_count': 711,\n",
      " 'log_count/DEBUG': 735,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 14,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/403': 1,\n",
      " 'scheduler/dequeued': 13,\n",
      " 'scheduler/dequeued/memory': 13,\n",
      " 'scheduler/enqueued': 13,\n",
      " 'scheduler/enqueued/memory': 13,\n",
      " 'start_time': datetime.datetime(2023, 5, 7, 7, 40, 52, 896075)}\n",
      "2023-05-07 14:40:54 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Remove file data collect if exist\n",
    "#!rm /content/fifa_crawler/fifa_crawler/dataset/teams_urls.json\n",
    "\n",
    "# Local notebook\n",
    "!rm /dataset/teams_urls.json\n",
    "\n",
    "# Crawl data\n",
    "!scrapy crawl teams_urls -o dataset/teams_urls.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsHYo26T2Lfp",
    "outputId": "23f05bd1-cbad-4456-aaf1-38b2392f4139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danh sách bao gồm 711 ID của các CLB.\n"
     ]
    }
   ],
   "source": [
    "# Google colab\n",
    "#path_teams_urls_json = \"/content/fifa_crawler/fifa_crawler/dataset/teams_urls.json\"\n",
    "\n",
    "# Local notebook\n",
    "path_teams_urls_json = \"dataset/teams_urls.json\"\n",
    "with open(path_teams_urls_json) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('Danh sách bao gồm', len(data), 'ID của các CLB.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj5VAkIVArRq"
   },
   "source": [
    "Sau khi đã có danh sách 712 ID của các CLB đã thu thập từ trang web SoFIFA, bạn sẽ tiến hành thu thập dữ liệu cụ thể của từng CLB ứng với các ID này bằng cách hoàn thành class `collect_team_info`(scrapy.Spider) như bên dưới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjS66tCS1Ov9"
   },
   "source": [
    "<b>Tạo file collect_teams_info.py</b>\n",
    "\n",
    "1. Thu thập thông tin chi tiết của mỗi đội bóng\n",
    "2. Trả về data dạng json\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"Name\": \"Chelsea\", \n",
    "    \"League\": \"[England] Premier League\", \n",
    "    \"Overall\": \"83\", \n",
    "    \"Attack\": \"83\", \n",
    "    \"Midfield\": \"81\", \n",
    "    \"Defence\": \"82\", \n",
    "    \"Home stadium\": \"Stamford Bridge\", \n",
    "    \"Rival team\": \"Arsenal\", \n",
    "    \"International prestige\": \"8\", \n",
    "    \"Domestic prestige\": \"9\", \n",
    "    \"Club worth\": \"\\n\", \n",
    "    \"Starting XI average age\": \"24.55\", \n",
    "    \"Whole team average age\": \"24.39\", \n",
    "    \"Captain\": \"R. James\", \n",
    "    \"Short free kick\": \"R. James\", \n",
    "    \"Long free kick\": \"E. Fernández\", \n",
    "    \"Left short free kick\": \"João Félix\", \n",
    "    \"Right short free kick\": \"R. James\", \n",
    "    \"Penalties\": \"K. Havertz\", \n",
    "    \"Left corner\": \"B. Chilwell\", \n",
    "    \"Right corner\": \"R. James\"\n",
    "},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7vHUkFitpbpv"
   },
   "outputs": [],
   "source": [
    "# \\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\n",
    "import scrapy\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "class collect_team_info(scrapy.Spider):\n",
    "  name='teams_info'\n",
    "  \n",
    "  def __init__(self):\n",
    "    try:\n",
    "      with open('dataset/teams_urls.json') as f:\n",
    "        self.teams = json.load(f)\n",
    "      self.team_count = 1\n",
    "    except IOError:\n",
    "      print(\"File not found\")\n",
    "\n",
    "  def start_requests(self):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    for team in self.teams:\n",
    "      # https://sofifa.com/team/10\n",
    "      url = 'https://sofifa.com' + team['team_url'] + '?units=mks'\n",
    "      yield scrapy.Request(url=url, headers=headers, callback=self.parse)\n",
    "  \n",
    "  def parse(self, response):\n",
    "      # YOUR CODE HERE\n",
    "      team_info = {}\n",
    "\n",
    "      team_info[\"Name\"] = response.css('div.info h1::text').get();\n",
    "\n",
    "      team_info[\"League\"] = response.css('div.meta.ellipsis a::text').get()\n",
    "\n",
    "      ratings = response.css('section.card .bp3-tag.p::text').getall()\n",
    "\n",
    "      team_info[\"Overall\"] = ratings[0]\n",
    "\n",
    "      team_info[\"Attack\"] = ratings[1]\n",
    "      \n",
    "      team_info[\"Midfield\"] = ratings[2]\n",
    "      \n",
    "      team_info[\"Defence\"] = ratings[3]\n",
    "\n",
    "      team_info[\"Home stadium\"] = response.css('ul.pl li.ellipsis:contains(\"Home stadium\")::text').get()\n",
    "      \n",
    "      team_info[\"Rival team\"] = response.css('ul.pl li:contains(\"Rival team\") a::text').get()\n",
    "\n",
    "      team_info[\"International prestige\"] = response.css('ul.pl li:contains(\"International prestige\") span::text').get()\n",
    "\n",
    "      team_info[\"Domestic prestige\"] = response.css('ul.pl li:contains(\"Domestic prestige\") span::text').get()\n",
    "\n",
    "      team_info[\"Club worth\"] =response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.]+)M')\n",
    "\n",
    "      team_info[\"Starting XI average age\"] = response.css('ul.pl li:contains(\"Starting XI average age\")::text').get()\n",
    "\n",
    "      team_info[\"Whole team average age\"] = response.css('ul.pl li:contains(\"Whole team average age\")::text').get()\n",
    "\n",
    "      team_info[\"Captain\"] = response.css('ul.pl li:contains(\"Captain\") a::text').get()\n",
    "\n",
    "      team_info[\"Short free kick\"] = response.css('ul.pl li:contains(\"Short free kick\") a::text').get()\n",
    "\n",
    "      team_info[\"Long free kick\"] = response.css('ul.pl li:contains(\"Long free kick\") a::text').get()\n",
    "\n",
    "      team_info[\"Left short free kick\"] = response.css('ul.pl li:contains(\"Left short free kick\") a::text').get()\n",
    "      \n",
    "      team_info[\"Right short free kick\"] = response.css('ul.pl li:contains(\"Right short free kick\") a::text').get()\n",
    "      \n",
    "      team_info[\"Penalties\"] = response.css('ul.pl li:contains(\"Penalties\") a::text').get()\n",
    "      \n",
    "      team_info[\"Left corner\"] = response.css('ul.pl li:contains(\"Left corner\") a::text').get()\n",
    "      \n",
    "      team_info[\"Right corner\"] = response.css('ul.pl li:contains(\"Right corner\") a::text').get()\n",
    "\n",
    "      yield team_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMtk8cmuqnXm"
   },
   "source": [
    "Sau khi đã hoàn thành class ở trên và lưu lại trong file `collect_teams_info.py`, các bạn sẽ tiếp tục chạy câu lệnh bên dưới để thu thập thông tin chi tiết của toàn bộ 712 CLB và xuất ra file `teams_info.json` ở cùng đường dẫn dataset như trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU9RnSzPMhUp"
   },
   "source": [
    "**Chú thích:**\n",
    "- Do trang web  `https://sofifa.com/` đang giới hạn số lượng yêu cầu trên mỗi khoảng thời gian để tránh quá tải. Việc này ảnh hưởng đến việc crawl dữ liệu không đầy đủ.\n",
    "\n",
    "```sh\n",
    "DOWNLOAD_DELAY = 0.5\n",
    "```\n",
    "\n",
    "Để khắc phục sự cố này, bạn có thể thêm độ trễ giữa mỗi yêu cầu để bạn không gửi quá nhiều yêu cầu trong một khoảng thời gian ngắn. Bạn có thể thực hiện việc này bằng cách sử dụng cài đặt `DOWNLOAD_DELAY` trong tệp `settings.py` của dự án Scrapy của mình. Ví dụ: bạn có thể đặt thành `DOWNLOAD_DELAY = 0.5` để đợi 0.5 giây giữa mỗi yêu cầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3ohWcdF1Ov-"
   },
   "source": [
    "<b>Start collect data từ file collect_teams_info.py</b>\n",
    "\n",
    "Data collect được lưu vào dataset/teams_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZeWsqMVMpNpW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "2023-05-07 14:41:10 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: fifa_crawler)\n",
      "2023-05-07 14:41:10 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform Windows-10-10.0.22000-SP0\n",
      "2023-05-07 14:41:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'fifa_crawler',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'fifa_crawler.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['fifa_crawler.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-05-07 14:41:10 [asyncio] DEBUG: Using selector: SelectSelector\n",
      "2023-05-07 14:41:10 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-05-07 14:41:10 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop\n",
      "2023-05-07 14:41:10 [scrapy.extensions.telnet] INFO: Telnet Password: a6926514cee83cd3\n",
      "2023-05-07 14:41:10 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-05-07 14:41:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-05-07 14:41:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-05-07 14:41:10 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-05-07 14:41:10 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-05-07 14:41:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-05-07 14:41:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-05-07 14:41:10 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://sofifa.com/robots.txt> (referer: None)\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 15 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 66 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 68 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 69 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 70 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 74 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:10 [protego] DEBUG: Rule at line 76 without any user agent to enforce it on.\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/10?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1318?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/21?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/243?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/10?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1318?units=mks>\n",
      "\n",
      "{'Name': 'England', 'League': '[World] Friendly International', 'Overall': '84', 'Attack': '85', 'Midfield': '83', 'Defence': '82', 'Home stadium': 'Wembley Stadium', 'Rival team': 'Germany', 'International prestige': '8', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.91', 'Whole team average age': '26.26', 'Captain': 'H. Kane', 'Short free kick': 'B. Saka', 'Long free kick': 'H. Kane', 'Left short free kick': 'H. Kane', 'Right short free kick': 'B. Saka', 'Penalties': 'H. Kane', 'Left corner': 'J. Grealish', 'Right corner': 'B. Saka'}\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/21?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/243?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/5?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/73?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/9?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/5?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/73?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/9?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1354?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1337?units=mks> (referer: None)\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1354?units=mks>\n",
      "\n",
      "{'Name': 'Portugal', 'League': '[World] Friendly International', 'Overall': '83', 'Attack': '84', 'Midfield': '84', 'Defence': '83', 'Home stadium': 'Waldstadion (Fussballstadion)', 'Rival team': 'Spain', 'International prestige': '8', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '29.36', 'Whole team average age': '26.39', 'Captain': 'Cristiano Ronaldo', 'Short free kick': 'Cristiano Ronaldo', 'Long free kick': 'Cristiano Ronaldo', 'Left short free kick': 'Cristiano Ronaldo', 'Right short free kick': 'Cristiano Ronaldo', 'Penalties': 'Cristiano Ronaldo', 'Left corner': 'Bruno Fernandes', 'Right corner': 'Bernardo Silva'}\n",
      "2023-05-07 14:41:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1337?units=mks>\n",
      "\n",
      "{'Name': 'Germany', 'League': '[World] Friendly International', 'Overall': '83', 'Attack': '80', 'Midfield': '84', 'Defence': '80', 'Home stadium': 'Stadion Olympik', 'Rival team': 'England', 'International prestige': '10', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.73', 'Whole team average age': '25.52', 'Captain': 'J. Kimmich', 'Short free kick': 'J. Kimmich', 'Long free kick': 'J. Kimmich', 'Left short free kick': 'J. Kimmich', 'Right short free kick': 'D. Raum', 'Penalties': 'N. Füllkrug', 'Left corner': 'J. Kimmich', 'Right corner': 'J. Kimmich'}\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1362?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/240?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/241?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1369?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1362?units=mks>\n",
      "\n",
      "{'Name': 'Spain', 'League': '[World] Friendly International', 'Overall': '83', 'Attack': '83', 'Midfield': '84', 'Defence': '83', 'Home stadium': 'Wanda Metropolitano', 'Rival team': 'Italy', 'International prestige': '9', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.55', 'Whole team average age': '25.91', 'Captain': 'Morata', 'Short free kick': 'Iago Aspas', 'Long free kick': 'Iago Aspas', 'Left short free kick': 'Iago Aspas', 'Right short free kick': 'Iago Aspas', 'Penalties': 'Morata', 'Left corner': 'Oyarzabal', 'Right corner': 'Iago Aspas'}\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/240?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/241?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1369?units=mks>\n",
      "\n",
      "{'Name': 'Argentina', 'League': '[World] Friendly International', 'Overall': '83', 'Attack': '84', 'Midfield': '81', 'Defence': '82', 'Home stadium': 'La Bombonera', 'Rival team': 'Germany', 'International prestige': '9', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '28.09', 'Whole team average age': '27.39', 'Captain': 'L. Messi', 'Short free kick': 'L. Messi', 'Long free kick': 'L. Messi', 'Left short free kick': 'L. Messi', 'Right short free kick': 'L. Messi', 'Penalties': 'L. Messi', 'Left corner': 'L. Messi', 'Right corner': 'L. Messi'}\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/44?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/11?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1335?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/44?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/11?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1335?units=mks>\n",
      "\n",
      "{'Name': 'France', 'League': '[World] Friendly International', 'Overall': '83', 'Attack': '85', 'Midfield': '83', 'Defence': '83', 'Home stadium': 'Orange Vélodrome', 'Rival team': 'Italy', 'International prestige': '10', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '24.64', 'Whole team average age': '24.91', 'Captain': 'K. Mbappé', 'Short free kick': 'A. Griezmann', 'Long free kick': 'A. Griezmann', 'Left short free kick': 'A. Griezmann', 'Right short free kick': 'A. Griezmann', 'Penalties': 'K. Mbappé', 'Left corner': 'A. Griezmann', 'Right corner': 'A. Griezmann'}\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/45?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/22?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/105035?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/18?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/45?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/22?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/105035?units=mks>\n",
      "\n",
      "{'Name': 'Netherlands', 'League': '[World] Friendly International', 'Overall': '82', 'Attack': '82', 'Midfield': '81', 'Defence': '82', 'Home stadium': 'Johan Cruijff ArenA', 'Rival team': 'Germany', 'International prestige': '8', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.82', 'Whole team average age': '26.48', 'Captain': 'V. van Dijk', 'Short free kick': 'M. Depay', 'Long free kick': 'M. Depay', 'Left short free kick': 'M. Depay', 'Right short free kick': 'M. Depay', 'Penalties': 'M. Depay', 'Left corner': 'M. Depay', 'Right corner': 'M. Depay'}\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/18?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112172?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/47?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112172?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/47?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1343?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/52?units=mks> (referer: None)\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1343?units=mks>\n",
      "\n",
      "{'Name': 'Italy', 'League': '[World] Friendly International', 'Overall': '82', 'Attack': '81', 'Midfield': '85', 'Defence': '81', 'Home stadium': 'Stadion Olympik', 'Rival team': 'France', 'International prestige': '9', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.73', 'Whole team average age': '26.26', 'Captain': 'M. Verratti', 'Short free kick': 'Jorginho', 'Long free kick': 'Jorginho', 'Left short free kick': 'Jorginho', 'Right short free kick': 'Jorginho', 'Penalties': 'Jorginho', 'Left corner': 'M. Verratti', 'Right corner': 'Jorginho'}\n",
      "2023-05-07 14:41:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/52?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/483?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/481?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1370?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/483?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/481?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/48?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/2?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/32?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1370?units=mks>\n",
      "\n",
      "{'Name': 'Brazil', 'League': '[World] Friendly International', 'Overall': '80', 'Attack': '81', 'Midfield': '80', 'Defence': '80', 'Home stadium': 'O Dromo', 'Rival team': 'Argentina', 'International prestige': '10', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '26.36', 'Whole team average age': '26.17', 'Captain': 'Adryan Zonta', 'Short free kick': 'Ronaldo Cabrais', 'Long free kick': 'Laure Santeiro', 'Left short free kick': 'Laure Santeiro', 'Right short free kick': 'Ronaldo Cabrais', 'Penalties': 'Oswaldinato', 'Left corner': 'Welington Dano', 'Right corner': 'Welington Dano'}\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/13?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/48?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/2?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/32?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/46?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/13?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/448?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/46?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1325?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/448?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/457?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/234?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/449?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/19?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1325?units=mks>\n",
      "\n",
      "{'Name': 'Belgium', 'League': '[World] Friendly International', 'Overall': '80', 'Attack': '81', 'Midfield': '78', 'Defence': '78', 'Home stadium': 'O Dromo', 'Rival team': 'Netherlands', 'International prestige': '8', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.18', 'Whole team average age': '26.22', 'Captain': 'J. Vertonghen', 'Short free kick': 'K. De Bruyne', 'Long free kick': 'K. De Bruyne', 'Left short free kick': 'K. De Bruyne', 'Right short free kick': 'K. De Bruyne', 'Penalties': 'R. Lukaku', 'Left corner': 'K. De Bruyne', 'Right corner': 'K. De Bruyne'}\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/457?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/234?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1328?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1824?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/449?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/19?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110374?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1328?units=mks>\n",
      "\n",
      "{'Name': 'Croatia', 'League': '[World] Friendly International', 'Overall': '79', 'Attack': '77', 'Midfield': '82', 'Defence': '78', 'Home stadium': 'Stadion Hanguk', 'Rival team': 'Hungary', 'International prestige': '6', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.64', 'Whole team average age': '26.96', 'Captain': 'L. Modri\\u0107', 'Short free kick': 'L. Modri\\u0107', 'Long free kick': 'L. Modri\\u0107', 'Left short free kick': 'L. Modri\\u0107', 'Right short free kick': 'L. Modri\\u0107', 'Penalties': 'A. Kramari\\u0107', 'Left corner': 'L. Modri\\u0107', 'Right corner': 'L. Modri\\u0107'}\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1824?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/39?units=mks> (referer: None)\n",
      "2023-05-07 14:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110374?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/39?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1331?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/236?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/237?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1331?units=mks>\n",
      "\n",
      "{'Name': 'Denmark', 'League': '[World] Friendly International', 'Overall': '78', 'Attack': '74', 'Midfield': '79', 'Defence': '78', 'Home stadium': 'Eastpoint Arena (Football Ground)', 'Rival team': 'Sweden', 'International prestige': '6', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.91', 'Whole team average age': '25.70', 'Captain': 'S. Kjær', 'Short free kick': 'M. Damsgaard', 'Long free kick': 'M. Damsgaard', 'Left short free kick': 'M. Damsgaard', 'Right short free kick': 'M. Damsgaard', 'Penalties': 'R. Højlund', 'Left corner': 'M. Damsgaard', 'Right corner': 'M. Damsgaard'}\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111111?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/236?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/237?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/111111?units=mks>\n",
      "\n",
      "{'Name': 'Morocco', 'League': '[World] Friendly International', 'Overall': '77', 'Attack': '77', 'Midfield': '76', 'Defence': '78', 'Home stadium': 'Stadion 23. Maj', 'Rival team': 'Qatar', 'International prestige': '3', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '26.36', 'Whole team average age': '26.09', 'Captain': 'R. Saïss', 'Short free kick': 'H. Ziyech', 'Long free kick': 'H. Ziyech', 'Left short free kick': 'H. Ziyech', 'Right short free kick': 'H. Ziyech', 'Penalties': 'S. Amallah', 'Left corner': 'H. Ziyech', 'Right corner': 'H. Ziyech'}\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/14?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/14?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/219?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1808?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/219?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1808?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/10029?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/66?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1860?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/10029?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/66?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1860?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/69?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/23?units=mks> (referer: None)\n",
      "2023-05-07 14:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/325?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/69?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/23?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/325?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1386?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1353?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/95?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/74?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1322?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1386?units=mks>\n",
      "\n",
      "{'Name': 'Mexico', 'League': '[World] Friendly International', 'Overall': '77', 'Attack': '78', 'Midfield': '77', 'Defence': '76', 'Home stadium': 'Stadion Olympik', 'Rival team': 'United States', 'International prestige': '7', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.64', 'Whole team average age': '26.43', 'Captain': 'G. Ochoa', 'Short free kick': 'L. Chávez', 'Long free kick': 'L. Chávez', 'Left short free kick': 'L. Chávez', 'Right short free kick': 'L. Chávez', 'Penalties': 'H. Lozano', 'Left corner': 'H. Lozano', 'Right corner': 'H. Lozano'}\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1353?units=mks>\n",
      "\n",
      "{'Name': 'Poland', 'League': '[World] Friendly International', 'Overall': '77', 'Attack': '79', 'Midfield': '76', 'Defence': '75', 'Home stadium': 'Waldstadion (Fussballstadion)', 'Rival team': 'Germany', 'International prestige': '5', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.64', 'Whole team average age': '25.52', 'Captain': 'R. Lewandowski', 'Short free kick': 'R. Lewandowski', 'Long free kick': 'P. Zieli\\u0144ski', 'Left short free kick': 'R. Lewandowski', 'Right short free kick': 'R. Lewandowski', 'Penalties': 'R. Lewandowski', 'Left corner': 'P. Zieli\\u0144ski', 'Right corner': 'P. Zieli\\u0144ski'}\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/95?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/74?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1322?units=mks>\n",
      "\n",
      "{'Name': 'Austria', 'League': '[World] Friendly International', 'Overall': '77', 'Attack': '78', 'Midfield': '78', 'Defence': '77', 'Home stadium': 'O Dromo', 'Rival team': 'Germany', 'International prestige': '4', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.82', 'Whole team average age': '25.87', 'Captain': 'M. Sabitzer', 'Short free kick': 'M. Sabitzer', 'Long free kick': 'N. Seiwald', 'Left short free kick': 'M. Sabitzer', 'Right short free kick': 'M. Sabitzer', 'Penalties': 'M. Sabitzer', 'Left corner': 'M. Sabitzer', 'Right corner': 'M. Sabitzer'}\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/452?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/452?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/7?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1043?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1799?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/8?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/25?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/245?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/7?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1043?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1799?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/8?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/25?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/245?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1330?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/450?units=mks> (referer: None)\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1330?units=mks>\n",
      "\n",
      "{'Name': 'Czech Republic', 'League': '[World] Friendly International', 'Overall': '76', 'Attack': '77', 'Midfield': '77', 'Defence': '75', 'Home stadium': 'Stadion Neder', 'Rival team': 'Germany', 'International prestige': '5', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '27.18', 'Whole team average age': '25.17', 'Captain': 'T. Sou\\u010dek', 'Short free kick': 'A. Barák', 'Long free kick': 'A. Barák', 'Left short free kick': 'A. Barák', 'Right short free kick': 'A. Barák', 'Penalties': 'A. Barák', 'Left corner': 'A. Hložek', 'Right corner': 'A. Hložek'}\n",
      "2023-05-07 14:41:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/450?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1896?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1876?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1896?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1876?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1352?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/64?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/65?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1363?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1352?units=mks>\n",
      "\n",
      "{'Name': 'Norway', 'League': '[World] Friendly International', 'Overall': '76', 'Attack': '82', 'Midfield': '77', 'Defence': '74', 'Home stadium': 'Eastpoint Arena (Football Ground)', 'Rival team': 'Sweden', 'International prestige': '5', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.45', 'Whole team average age': '24.91', 'Captain': 'M. Ødegaard', 'Short free kick': 'M. Ødegaard', 'Long free kick': 'M. Ødegaard', 'Left short free kick': 'M. Ødegaard', 'Right short free kick': 'M. Ødegaard', 'Penalties': 'E. Haaland', 'Left corner': 'M. Ødegaard', 'Right corner': 'M. Ødegaard'}\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/64?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/65?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1363?units=mks>\n",
      "\n",
      "{'Name': 'Sweden', 'League': '[World] Friendly International', 'Overall': '76', 'Attack': '78', 'Midfield': '77', 'Defence': '75', 'Home stadium': 'Waldstadion (Fussballstadion)', 'Rival team': 'Denmark', 'International prestige': '5', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.91', 'Whole team average age': '26.48', 'Captain': 'V. Lindelöf', 'Short free kick': 'E. Forsberg', 'Long free kick': 'E. Forsberg', 'Left short free kick': 'E. Forsberg', 'Right short free kick': 'E. Forsberg', 'Penalties': 'E. Forsberg', 'Left corner': 'E. Forsberg', 'Right corner': 'E. Forsberg'}\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1925?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/72?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/144?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1925?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/72?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/144?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/436?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/175?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/436?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/175?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/461?units=mks> (referer: None)\n",
      "2023-05-07 14:41:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111811?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/461?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111811?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/383?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/247?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1035?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/383?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/247?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1035?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/479?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1831?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110062?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/479?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110062?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1831?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/55?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/71?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/480?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/55?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/17?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/327?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/71?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/480?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/17?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/327?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1366?units=mks> (referer: None)\n",
      "2023-05-07 14:41:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1877?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1359?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1366?units=mks>\n",
      "\n",
      "{'Name': 'Ukraine', 'League': '[World] Friendly International', 'Overall': '75', 'Attack': '74', 'Midfield': '77', 'Defence': '72', 'Home stadium': 'Stadion Olympik', 'Rival team': 'Wales', 'International prestige': '6', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.64', 'Whole team average age': '25.61', 'Captain': 'A. Yarmolenko', 'Short free kick': 'R. Malinovskyi', 'Long free kick': 'R. Malinovskyi', 'Left short free kick': 'R. Malinovskyi', 'Right short free kick': 'R. Malinovskyi', 'Penalties': 'R. Malinovskyi', 'Left corner': 'R. Malinovskyi', 'Right corner': 'R. Malinovskyi'}\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1877?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/326?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/169?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111462?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1387?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1359?units=mks>\n",
      "\n",
      "{'Name': 'Scotland', 'League': '[World] Friendly International', 'Overall': '75', 'Attack': '72', 'Midfield': '75', 'Defence': '76', 'Home stadium': 'Stadion Olympik', 'Rival team': 'England', 'International prestige': '4', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '25.82', 'Whole team average age': '26.13', 'Captain': 'A. Robertson', 'Short free kick': 'C. McGregor', 'Long free kick': 'C. McGregor', 'Left short free kick': 'C. McGregor', 'Right short free kick': 'C. McGregor', 'Penalties': 'C. Adams', 'Left corner': 'J. McGinn', 'Right corner': 'J. McGinn'}\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/326?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/169?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111974?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/111462?units=mks>\n",
      "\n",
      "{'Name': 'Ghana', 'League': '[World] Friendly International', 'Overall': '75', 'Attack': '81', 'Midfield': '75', 'Defence': '74', 'Home stadium': 'Stadio Classico', 'Rival team': 'United States', 'International prestige': '5', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '26.00', 'Whole team average age': '24.52', 'Captain': 'T. Partey', 'Short free kick': 'T. Partey', 'Long free kick': 'T. Partey', 'Left short free kick': 'T. Partey', 'Right short free kick': 'T. Partey', 'Penalties': 'T. Partey', 'Left corner': 'J. Ayew', 'Right corner': 'J. Ayew'}\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1387?units=mks>\n",
      "\n",
      "{'Name': 'United States', 'League': '[World] Friendly International', 'Overall': '75', 'Attack': '74', 'Midfield': '75', 'Defence': '74', 'Home stadium': 'Dignity Health Sports Park', 'Rival team': 'Mexico', 'International prestige': '6', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '23.82', 'Whole team average age': '23.17', 'Captain': 'C. Pulisic', 'Short free kick': 'C. Pulisic', 'Long free kick': 'W. McKennie', 'Left short free kick': 'C. Pulisic', 'Right short free kick': 'C. Pulisic', 'Penalties': 'C. Pulisic', 'Left corner': 'C. Pulisic', 'Right corner': 'C. Pulisic'}\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1968?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111974?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1968?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/453?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/189?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/231?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/462?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/246?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/453?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/189?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/231?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/462?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/278?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/246?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/278?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1048?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/36?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/266?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1048?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/31?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/36?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110373?units=mks> (referer: None)\n",
      "2023-05-07 14:41:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/266?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/31?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/100409?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110373?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/54?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1861?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/100409?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/54?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1861?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/70?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/97?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1819?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/70?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/97?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1819?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/77?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/76?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/322?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/77?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/76?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/81?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/322?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1039?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/91?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/81?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1039?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112224?units=mks> (referer: None)\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/91?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112224?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112494?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111722?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/94?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112494?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111722?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/94?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/897?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/897?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1915?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1926?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1915?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/898?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1926?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113796?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111768?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/898?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110500?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113796?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/433?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114611?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112791?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/647?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1474?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110968?units=mks> (referer: None)\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/199?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112791?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1478?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/711?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/647?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112670?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110968?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/28?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110998?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111768?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110982?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/433?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110500?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1334?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/57?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114611?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110998?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110636?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1474?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110982?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/199?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1478?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/711?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112670?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111674?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/28?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111768?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111768?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110395?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110500?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110500?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1334?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/433?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/433?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113044?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111768?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/57?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114611?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114611?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110500?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110636?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/433?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1478?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1478?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1474?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1474?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112670?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112670?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/199?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/199?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/711?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/711?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113044?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114611?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/28?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/28?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111674?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1478?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1474?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111434?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112670?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/199?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/711?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1356?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110395?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110935?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1334?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1334?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1874?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/28?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/607?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/57?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/57?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110636?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110636?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1334?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1888?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/57?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1903?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110636?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110980?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111674?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111674?units=mks> (referer: None)\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111715?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111434?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111674?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1356?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110395?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110395?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110935?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1415?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110395?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/607?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101059?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/900?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1888?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1903?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101016?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/109?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1874?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110980?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111052?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111715?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111434?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111434?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1356?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1356?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101083?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111434?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1415?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1356?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101059?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/900?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/607?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/607?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1888?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1888?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110935?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110935?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1903?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1903?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101016?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/607?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1888?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110935?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/109?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1874?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1874?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110980?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110980?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1903?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111715?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111715?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110556?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111052?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111325?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101083?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1874?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110980?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111715?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1415?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1415?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/900?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/900?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101059?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101059?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2014?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/741?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111339?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101100?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101016?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101016?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1415?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/900?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101059?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/109?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/109?units=mks> (referer: None)\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101101?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101105?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113142?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110556?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111052?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111052?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101016?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111325?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/109?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101083?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101083?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111052?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/3?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1530?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112134?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2014?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/741?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111339?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101083?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101100?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/263?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1800?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101101?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113142?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110556?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110556?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101105?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1801?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111325?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111325?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110093?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110556?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1530?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/3?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112134?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/741?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/741?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/2014?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/2014?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111339?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111339?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111325?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101100?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101100?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/741?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/2014?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111339?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101105?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101105?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101101?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101101?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1800?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113142?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113142?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111117?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101100?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1801?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/614?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101105?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101101?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113142?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/873?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1530?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1530?units=mks> (referer: None)\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1898?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:23 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110975?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1530?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111117?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112001?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110093?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/263?units=mks> (referer: None)\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112513?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1800?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1800?units=mks> (referer: None)\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/614?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110981?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1923?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/263?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1801?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1801?units=mks> (referer: None)\n",
      "2023-05-07 14:41:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1800?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/873?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1801?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110975?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110986?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112001?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111117?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111117?units=mks> (referer: None)\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110093?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110093?units=mks> (referer: None)\n",
      "2023-05-07 14:41:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111117?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110093?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/918?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112513?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101007?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1923?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110981?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1898?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1952?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110986?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/614?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/614?units=mks> (referer: None)\n",
      "2023-05-07 14:41:24 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/873?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:24 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/873?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/614?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/873?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112001?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112001?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110975?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110975?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112513?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112513?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111008?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/918?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111010?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112001?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101007?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110975?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112513?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1923?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1923?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1952?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110981?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110981?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1923?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110981?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110502?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101033?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111019?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112809?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1898?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1898?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111008?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/918?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/918?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/688?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111010?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110986?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110986?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1898?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101007?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101007?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/918?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110986?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101007?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1952?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:25 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1952?units=mks> (referer: None)\n",
      "2023-05-07 14:41:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111539?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1473?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101033?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110502?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1952?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112809?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111019?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112578?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/744?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111008?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111008?units=mks> (referer: None)\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/688?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110827?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111008?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101103?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111539?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101104?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1473?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111010?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111010?units=mks> (referer: None)\n",
      "2023-05-07 14:41:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111010?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110502?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110502?units=mks> (referer: None)\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111019?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111019?units=mks> (referer: None)\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112578?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112809?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112809?units=mks> (referer: None)\n",
      "2023-05-07 14:41:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110502?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111019?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112809?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/744?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/688?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:26 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/688?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100851?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101033?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101033?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110827?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101103?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/688?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101033?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111539?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111539?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101104?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1473?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1473?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111539?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/244?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111724?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101108?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1909?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1473?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112578?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112578?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112578?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/744?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/744?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1913?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100851?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1910?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101103?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101103?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/744?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101103?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110827?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110827?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101104?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101104?units=mks> (referer: None)\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1919?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101108?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/634?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110827?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101104?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111724?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1909?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/244?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/896?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1910?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1913?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100851?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100851?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110724?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100851?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110738?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110984?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/634?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1919?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101108?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101108?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101108?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/244?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/244?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1909?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1909?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111724?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111724?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/896?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1910?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1910?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/244?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1909?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111724?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1910?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110724?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101020?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1913?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1913?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110984?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110738?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1913?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/634?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/634?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1919?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1919?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112540?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/670?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1438?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/634?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1919?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/896?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/896?units=mks> (referer: None)\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/159?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101020?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111013?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110724?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:28 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110724?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/896?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111235?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110724?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110984?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110984?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111014?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110738?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110738?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1960?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/171?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/670?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111235?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110984?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110738?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/687?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112540?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/159?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1438?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111013?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101020?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101020?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101020?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101041?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/691?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111014?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/697?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/696?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1960?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/171?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/670?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/670?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/687?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1438?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1438?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/159?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/159?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112540?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112540?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/670?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1438?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/159?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112540?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1477?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111013?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111013?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101041?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/691?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111014?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111014?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/696?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111013?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/697?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111014?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1960?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1960?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/171?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/171?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/459?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111817?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/687?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/687?units=mks> (referer: None)\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1960?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/171?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/205?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/687?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1477?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114640?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/209?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/691?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/691?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101041?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101041?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1745?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/691?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101041?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/696?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/696?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/697?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/697?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/459?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112472?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/78?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111817?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/696?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/697?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1906?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/205?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/209?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1477?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1477?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114640?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1943?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1477?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/166?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1745?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1746?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/459?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/459?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/468?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112472?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/459?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/78?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111817?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111817?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1906?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/209?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/209?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/205?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/205?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101085?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111817?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114640?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114640?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1943?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/209?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/205?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/166?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1746?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1745?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1745?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114640?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/468?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112472?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112472?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1796?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/78?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/78?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1745?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1906?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1906?units=mks> (referer: None)\n",
      "2023-05-07 14:41:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/267?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112472?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/78?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112139?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1041?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101085?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1943?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1943?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1906?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1053?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/166?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/166?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1943?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1746?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1746?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/38?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/468?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/468?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1853?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1796?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/166?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1746?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/86?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/468?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/598?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112139?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/267?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1041?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101085?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101085?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1367?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/347?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1053?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1884?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101085?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/38?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/605?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1853?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1796?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1796?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/598?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/86?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112139?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112139?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/267?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/267?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1796?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1886?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1041?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1041?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1367?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112139?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/347?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/267?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1053?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1053?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1884?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1041?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/38?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/38?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/605?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1853?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1853?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1053?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/86?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/86?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111455?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/598?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:31 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/598?units=mks> (referer: None)\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/38?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1853?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/378?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/86?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1886?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/379?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/598?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110468?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1367?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1367?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/347?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/347?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/393?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1884?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1884?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1367?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110741?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/605?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/605?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/347?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/160?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/673?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1884?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111455?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/605?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/206?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/379?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/378?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1886?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1886?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110468?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/472?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1886?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/230?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/217?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/393?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110741?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110832?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/160?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/206?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/378?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/378?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111455?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111455?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/673?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/379?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/379?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1792?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/378?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111455?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/379?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110468?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110468?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/472?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/230?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/393?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/393?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110468?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110741?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110741?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/393?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110832?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/160?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/160?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1794?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/206?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:32 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/206?units=mks> (referer: None)\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110741?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/160?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/673?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/673?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/206?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1795?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/12?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/456?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1792?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/217?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/673?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1744?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/472?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/472?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/217?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/230?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/230?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112585?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1738?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110832?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110832?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/472?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1794?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/230?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/983?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110832?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111065?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1795?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/456?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/12?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1792?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1792?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101084?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1744?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2013?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/224?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1738?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1792?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112585?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111328?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1794?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1794?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/983?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1795?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1795?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111065?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/456?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/456?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1794?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/12?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/12?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1795?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112096?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101084?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/456?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/12?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2013?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1744?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1744?units=mks> (referer: None)\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/224?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1738?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:33 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1738?units=mks> (referer: None)\n",
      "2023-05-07 14:41:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1744?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1738?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112585?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112585?units=mks> (referer: None)\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111328?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111329?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/983?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/983?units=mks> (referer: None)\n",
      "2023-05-07 14:41:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112585?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/983?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101109?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111065?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111065?units=mks> (referer: None)\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112096?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101084?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101084?units=mks> (referer: None)\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110069?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111065?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101084?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1786?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1788?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/2013?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/2013?units=mks> (referer: None)\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/523?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/224?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:34 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/224?units=mks> (referer: None)\n",
      "2023-05-07 14:41:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/2013?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/224?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2023?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1805?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101109?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111329?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/270?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111328?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111328?units=mks> (referer: None)\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110069?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111328?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112914?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112096?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112096?units=mks> (referer: None)\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/523?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112096?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1788?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1786?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100628?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1814?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2023?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1805?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111329?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111329?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101109?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101109?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/270?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111329?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101109?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110069?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110069?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1816?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112914?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110069?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112408?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100628?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1788?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1788?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/523?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/523?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1786?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1786?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1788?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/523?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1786?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1814?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1805?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1805?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/2023?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/2023?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/33?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111400?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/270?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/270?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1816?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1805?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/2023?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112914?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112914?units=mks> (referer: None)\n",
      "2023-05-07 14:41:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/298?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/270?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100628?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100628?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/299?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112408?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112427?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112914?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/300?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100628?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1814?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1814?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110902?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111400?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/33?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/573?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1814?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113981?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1816?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1816?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1806?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/298?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/299?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1816?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112427?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/300?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1815?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110902?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111400?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111400?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/33?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/33?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/573?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113981?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111400?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111651?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1806?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/33?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/298?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/298?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/299?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/299?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112427?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112427?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/300?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/300?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/298?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/299?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110902?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110902?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1815?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112427?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/300?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10020?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111140?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/573?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:37 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/573?units=mks> (referer: None)\n",
      "2023-05-07 14:41:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110902?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111651?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113981?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113981?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/573?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1806?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1806?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113981?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/15?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/15?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111144?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10031?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1806?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111657?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/50?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/15?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1815?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1815?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1842?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10020?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111140?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110396?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112408?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1815?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10031?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1341?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111651?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111651?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110406?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111144?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112408?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114510?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111651?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/50?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111657?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1842?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/80?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/10020?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/10020?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111140?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111140?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/10020?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111710?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110396?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1341?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1887?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111140?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/10031?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/10031?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110406?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/10031?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111144?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111144?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114510?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111657?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111657?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/50?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:38 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/50?units=mks> (referer: None)\n",
      "2023-05-07 14:41:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/80?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1842?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1842?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111144?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111657?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/50?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111711?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101110?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111710?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1842?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110396?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110396?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1887?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1341?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1341?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110396?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1341?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/252?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110406?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110406?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1793?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114510?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114510?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110081?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/260?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110406?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114510?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111711?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101110?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/80?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/80?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/518?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111710?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111710?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/269?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/80?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1887?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1887?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111710?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/29?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/252?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100135?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1793?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1832?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1887?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110081?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/260?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/518?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111711?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111711?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101110?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101110?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10030?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111711?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101110?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1843?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/269?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/252?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/252?units=mks> (referer: None)\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100135?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/29?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1847?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1793?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1793?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/252?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1832?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/518?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/518?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110081?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110081?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/260?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/260?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1848?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1793?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111928?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/518?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110081?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/260?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10030?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1843?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/29?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/29?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1847?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100135?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100135?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/269?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/269?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/29?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100135?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/320?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/576?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1832?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1832?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/324?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112965?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/269?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1848?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1832?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112716?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111928?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/10030?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/10030?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1847?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1847?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1843?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1843?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110930?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/10030?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/88?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111701?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1847?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1843?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/320?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/324?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/576?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10846?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112965?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1848?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1848?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112716?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111928?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111928?units=mks> (referer: None)\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110178?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1892?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1848?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111928?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1750?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110930?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/88?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111701?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/324?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/324?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/320?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/320?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/576?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/576?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/324?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/320?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112965?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112965?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112716?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112716?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10846?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112606?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/226?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/576?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112965?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112716?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1892?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1750?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/88?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/88?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110178?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110930?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110930?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111701?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111701?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/88?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110930?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111701?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/485?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114161?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111334?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/10846?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/10846?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1516?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/232?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/226?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/10846?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114162?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1750?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1750?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1892?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1892?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110178?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110178?units=mks> (referer: None)\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112606?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110580?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1750?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1892?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110178?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112885?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/232?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111334?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1516?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/485?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114162?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100087?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112606?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112606?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111334?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1516?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/226?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/226?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/485?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112606?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110329?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/226?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110580?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110839?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112885?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/232?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/232?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112893?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113149?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112390?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/232?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100087?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114162?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114162?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/271?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100888?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114162?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110329?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110580?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110580?units=mks> (referer: None)\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110839?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112885?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:42 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112885?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/114161?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110580?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100087?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100087?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/27?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1013?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113149?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112885?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112893?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112390?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/114161?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100888?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100087?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1013?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/271?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111138?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110329?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110329?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100646?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112689?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110839?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110839?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110329?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110839?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1809?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/34?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/27?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/570?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113149?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113149?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112893?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112893?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113149?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112893?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100888?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100888?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112390?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112390?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100646?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/271?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/271?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111138?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/294?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100888?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112390?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/271?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1809?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/570?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112689?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/34?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1837?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/27?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:43 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/27?units=mks> (referer: None)\n",
      "2023-05-07 14:41:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/27?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/819?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111138?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111138?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/567?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100646?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100646?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111138?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100646?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112996?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1355?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112689?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112689?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/294?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1809?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1809?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1908?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1837?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/570?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/570?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112689?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1809?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/570?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/567?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101014?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/34?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/34?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/819?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101047?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/674?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/34?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/294?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/294?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1908?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/191?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/294?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/211?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1837?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:44 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1837?units=mks> (referer: None)\n",
      "2023-05-07 14:41:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101014?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1837?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/567?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/567?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/467?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/674?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112996?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1355?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/463?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/567?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101047?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1908?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1908?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/191?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/819?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/819?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/211?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1908?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/819?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101014?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101014?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/517?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/229?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/467?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101014?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/674?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/674?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/674?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1355?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1355?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101088?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112996?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112996?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/463?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101047?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:45 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101047?units=mks> (referer: None)\n",
      "2023-05-07 14:41:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1355?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/191?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/191?units=mks> (referer: None)\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112908?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111716?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112996?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101047?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/211?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/211?units=mks> (referer: None)\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/191?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/211?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1893?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/517?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/229?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/467?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/467?units=mks> (referer: None)\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112744?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/467?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101088?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/463?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/463?units=mks> (referer: None)\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/106?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/62?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/463?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/59?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111716?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/68?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/319?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112908?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1893?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112744?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/229?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/229?units=mks> (referer: None)\n",
      "2023-05-07 14:41:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/517?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:46 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/517?units=mks> (referer: None)\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/229?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/517?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101088?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101088?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113217?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/106?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/62?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101088?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/68?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111716?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111716?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/59?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111716?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/319?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1893?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1893?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112908?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112908?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116295?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112744?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112744?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1893?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112908?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/89?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113217?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112744?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114023?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/62?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/62?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/106?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/106?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/62?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/106?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/59?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/59?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1905?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/68?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:47 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/68?units=mks> (referer: None)\n",
      "2023-05-07 14:41:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111473?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/319?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/319?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/59?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/68?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/319?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110456?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116295?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110969?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/89?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113217?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113217?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113217?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114023?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113029?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1905?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114554?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110978?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111239?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111473?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110969?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1929?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110456?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/89?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/89?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/116295?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/116295?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110989?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114023?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114023?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113029?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/89?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/116295?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1905?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1905?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114554?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114023?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110978?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111239?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111473?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111473?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1905?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110456?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110456?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1929?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111473?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110969?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110969?units=mks> (referer: None)\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110993?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110990?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110456?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110989?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110969?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113029?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113029?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1938?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114554?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114554?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114582?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113029?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110978?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110978?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111239?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111239?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114554?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/920?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110745?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1929?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1929?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110978?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111239?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113057?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110993?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1929?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110990?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110989?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110989?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110765?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1938?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112558?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110989?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114582?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1971?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110776?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110745?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/920?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/700?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113057?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110993?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110993?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/190?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110990?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110990?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110993?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1938?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1938?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110765?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112558?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110990?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114582?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114582?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1971?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1938?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/920?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/920?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110745?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110745?units=mks> (referer: None)\n",
      "2023-05-07 14:41:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110776?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114582?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/700?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113057?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113057?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/920?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110745?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/190?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/708?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/702?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113057?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110532?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110765?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110765?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112558?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112558?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111822?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1971?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1971?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110765?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113378?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110776?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110776?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112558?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1971?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110569?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/700?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/700?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111086?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110776?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/190?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/190?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/702?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/708?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/700?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110532?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110831?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/190?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111088?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111822?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/503?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112120?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113378?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110569?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111086?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110075?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/4?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/702?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/702?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110532?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110532?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/708?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/708?units=mks> (referer: None)\n",
      "2023-05-07 14:41:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110831?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/702?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111822?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111822?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111088?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110532?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/708?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/503?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112120?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111822?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113378?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113378?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110569?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110569?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111086?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111086?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110075?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/4?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113378?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110569?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111086?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115716?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2055?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2056?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110831?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110831?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111088?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111088?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/780?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110831?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111088?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/503?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/503?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112120?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112120?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1813?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/272?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/503?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113182?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112120?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/4?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/4?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110075?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110075?units=mks> (referer: None)\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115716?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2055?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2056?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/4?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110075?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115494?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116007?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/780?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111659?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110645?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1813?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/272?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113182?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112695?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/58?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/115716?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/115716?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/2055?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/2055?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115494?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/2056?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/2056?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/115716?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/2055?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116007?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111659?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/780?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/780?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/2056?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110645?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1813?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1813?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/780?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/272?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/272?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113182?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113182?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112695?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1813?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/58?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110394?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/272?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113182?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/580?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/115494?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/115494?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110955?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/116007?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/116007?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111659?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111659?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/115494?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110967?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110645?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110645?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/116007?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111993?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111659?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111766?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110970?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110645?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112695?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112695?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/58?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/58?units=mks> (referer: None)\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110394?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/580?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/127?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112695?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/58?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110955?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112511?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/645?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110967?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116360?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111993?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111766?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110970?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1933?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1932?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110394?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110394?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/580?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/580?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/127?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112511?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110955?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110955?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110394?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/580?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116360?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/645?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110967?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110967?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110955?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111993?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111993?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111766?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111766?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110967?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1932?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111993?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1933?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111766?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110970?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110970?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114580?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110747?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110970?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/127?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/127?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111004?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112511?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112511?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/645?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/645?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/116360?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/116360?units=mks> (referer: None)\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/127?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112511?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/645?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110749?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110750?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/116360?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1932?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1932?units=mks> (referer: None)\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1933?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1933?units=mks> (referer: None)\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1951?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1932?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1933?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114580?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110751?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113058?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110747?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111004?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1443?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/420?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110750?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112552?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110749?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114600?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/181?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1951?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114580?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114580?units=mks> (referer: None)\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110751?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110747?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:54 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110747?units=mks> (referer: None)\n",
      "2023-05-07 14:41:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114580?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110747?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/420?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111004?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111004?units=mks> (referer: None)\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1443?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110750?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110750?units=mks> (referer: None)\n",
      "2023-05-07 14:41:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111004?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113058?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112552?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110750?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114600?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110749?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110749?units=mks> (referer: None)\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/181?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110751?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110751?units=mks> (referer: None)\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1951?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1951?units=mks> (referer: None)\n",
      "2023-05-07 14:41:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110749?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110751?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1951?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112572?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15040?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/420?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/420?units=mks> (referer: None)\n",
      "2023-05-07 14:41:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/420?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2017?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1443?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:55 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1443?units=mks> (referer: None)\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114147?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112552?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112552?units=mks> (referer: None)\n",
      "2023-05-07 14:41:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1443?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113058?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113058?units=mks> (referer: None)\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111332?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114600?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114600?units=mks> (referer: None)\n",
      "2023-05-07 14:41:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112552?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113058?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114600?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/487?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/181?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/181?units=mks> (referer: None)\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101097?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15040?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112572?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/181?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/746?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/492?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/2017?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114147?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110321?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111091?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111332?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112115?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/487?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:56 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101097?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111092?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/15040?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/15040?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/746?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112572?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112572?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/492?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/15040?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112572?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110321?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/2017?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/2017?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114147?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114147?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112115?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/2017?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114147?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111091?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111332?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111332?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/487?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/487?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101097?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:57 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101097?units=mks> (referer: None)\n",
      "2023-05-07 14:41:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111092?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111332?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/487?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101097?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/506?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/492?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/492?units=mks> (referer: None)\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110321?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110321?units=mks> (referer: None)\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/746?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/746?units=mks> (referer: None)\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1787?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1790?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/492?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110321?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/746?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110078?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111092?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111092?units=mks> (referer: None)\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112115?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112115?units=mks> (referer: None)\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111091?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111091?units=mks> (referer: None)\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110592?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111092?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110597?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112115?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111091?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1798?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/506?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1944?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1787?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111769?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1947?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1790?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110078?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115358?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15009?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111779?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110592?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110597?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1798?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/506?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:58 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/506?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1944?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1787?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1787?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111769?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/506?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1947?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1787?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1790?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1790?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110078?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110078?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115358?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15009?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1790?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111779?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110078?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110592?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110592?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110597?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110597?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1798?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1798?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1446?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110592?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1944?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1944?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110597?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1798?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1713?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111769?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111769?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1947?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1947?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113345?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1944?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111769?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100804?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1947?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/15009?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/15009?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100805?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111779?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111779?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/115358?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/115358?units=mks> (referer: None)\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/15009?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1755?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111779?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100325?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/115358?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1446?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111083?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1713?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113387?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112883?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110581?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113345?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:41:59 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100804?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100805?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113926?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111395?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100325?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1755?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1446?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1446?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111083?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1713?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1713?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113387?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112883?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1446?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110581?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100804?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100804?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1713?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113345?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113345?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100805?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100805?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113926?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100804?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113345?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100325?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100325?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111395?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100805?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1755?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1755?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111083?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111083?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112163?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100325?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1755?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111396?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111083?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113387?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113387?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112883?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112883?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113387?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111398?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112883?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111399?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113926?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113926?units=mks> (referer: None)\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/561?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113926?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/306?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113458?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112163?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110676?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111396?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110581?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110581?units=mks> (referer: None)\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112985?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110581?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111395?units=mks> (referer: None)\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111399?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/357?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/561?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111395?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/306?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112163?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112163?units=mks> (referer: None)\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113458?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112163?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110676?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111398?units=mks> (referer: None)\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111396?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111396?units=mks> (referer: None)\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/361?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110194?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111398?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110691?units=mks> (referer: None)\n",
      "2023-05-07 14:42:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111396?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/306?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/306?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112985?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111399?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111399?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/357?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110691?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/306?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111399?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1928?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110676?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110676?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113458?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113458?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1930?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110676?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113458?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/149?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/361?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100759?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110194?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/922?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/357?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/357?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/561?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/357?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1930?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/561?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111774?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1928?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1439?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/149?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/361?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:02 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/361?units=mks> (referer: None)\n",
      "2023-05-07 14:42:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100759?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/361?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110194?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110194?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/922?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/15001?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110752?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111774?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110194?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/15001?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1930?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1930?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/418?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1439?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1930?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110501?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/149?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/149?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100759?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100759?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112985?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/149?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/781?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110752?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/922?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/922?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100759?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1928?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112985?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/526?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1439?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1439?units=mks> (referer: None)\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/922?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1928?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1569?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1439?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110501?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1825?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111393?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/301?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/781?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112199?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110752?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110752?units=mks> (referer: None)\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/82?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110752?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/83?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/526?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110501?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110501?units=mks> (referer: None)\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1825?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111774?units=mks> (referer: None)\n",
      "2023-05-07 14:42:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110501?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111774?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1569?units=mks> (referer: None)\n",
      "2023-05-07 14:42:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/418?units=mks> (referer: None)\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/301?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114004?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1569?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/418?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/781?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/781?units=mks> (referer: None)\n",
      "2023-05-07 14:42:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/781?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111393?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112199?units=mks> (referer: None)\n",
      "2023-05-07 14:42:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112199?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/526?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/526?units=mks> (referer: None)\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111707?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1825?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1825?units=mks> (referer: None)\n",
      "2023-05-07 14:42:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/526?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1825?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/83?units=mks> (referer: None)\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/621?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114004?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/83?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/631?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/82?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112505?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/301?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/301?units=mks> (referer: None)\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112260?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/301?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112990?units=mks> (referer: None)\n",
      "2023-05-07 14:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112990?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/143?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111707?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110482?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111393?units=mks> (referer: None)\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/82?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/82?units=mks> (referer: None)\n",
      "2023-05-07 14:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111393?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/82?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114004?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114004?units=mks> (referer: None)\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/631?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/621?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114004?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1936?units=mks> (referer: None)\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1958?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112260?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15015?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1936?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/143?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111707?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111707?units=mks> (referer: None)\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1962?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111707?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112505?units=mks> (referer: None)\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1463?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112505?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110482?units=mks> (referer: None)\n",
      "2023-05-07 14:42:07 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15015?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1756?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110482?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/631?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/631?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/114604?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1958?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/143?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/143?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/114604?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1962?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112260?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112260?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/143?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113380?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113892?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/621?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112260?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110313?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1463?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113146?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1756?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/621?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/15015?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/15015?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110587?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1958?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1958?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112126?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/15015?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1802?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1962?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1962?units=mks> (referer: None)\n",
      "2023-05-07 14:42:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1958?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1803?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1962?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113146?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110313?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1463?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1463?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1756?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1756?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1804?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1463?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110587?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1756?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113459?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112126?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1802?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113380?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1803?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113743?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110313?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110313?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113380?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113146?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113146?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110313?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1804?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1931?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/381?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113146?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113459?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112126?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112126?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1802?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1802?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112126?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1802?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1934?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113892?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1803?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1803?units=mks> (referer: None)\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113743?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113300?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113892?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1803?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/381?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110587?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113459?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113459?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111773?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110587?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1931?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1940?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113459?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1480?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113743?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113743?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1456?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1934?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1931?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1940?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113743?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:10 [scrapy.extensions.logstats] INFO: Crawled 600 pages (at 600 pages/min), scraped 23 items (at 23 items/min)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113300?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112072?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1804?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/381?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/381?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1757?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1596?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/381?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1804?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110890?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/92?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111773?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1456?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1480?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112222?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1934?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1934?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112072?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113300?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113300?units=mks> (referer: None)\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/121?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1934?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113300?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112254?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1596?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1757?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110890?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/92?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111773?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111773?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1456?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1456?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1480?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1480?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112222?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111773?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1456?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1480?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/142?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112072?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112072?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112254?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/121?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112072?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1596?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1596?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1757?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1757?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110890?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110890?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/92?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/92?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112259?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1596?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1757?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/142?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110890?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1935?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113299?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/92?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112259?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112222?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112222?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113302?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/121?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/121?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112254?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112254?units=mks> (referer: None)\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112222?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/423?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/121?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112254?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/432?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/445?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114628?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15048?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1935?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113299?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113301?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112378?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110799?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113301?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/981?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113302?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/423?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/432?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/142?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114628?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/445?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1935?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1935?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15048?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/142?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115489?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1935?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110799?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112378?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/981?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113302?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113302?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/432?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/432?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/423?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/423?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/305?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113302?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114628?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114628?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/432?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/423?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113299?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114628?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112378?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112378?units=mks> (referer: None)\n",
      "2023-05-07 14:42:12 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/115489?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113299?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112378?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110799?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110799?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/981?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/981?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/15048?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/837?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110799?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/981?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1955?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/305?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113298?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1941?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/15048?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/837?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/563?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113297?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111629?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/115489?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/115489?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/834?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/115489?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112541?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113257?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111839?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1955?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/305?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/305?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1941?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113298?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/305?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/563?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113297?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/445?units=mks> (referer: None)\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111629?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114168?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112541?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/445?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/834?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/113257?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111839?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1955?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:13 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1955?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1955?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1941?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1941?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113298?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113298?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1941?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113298?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/563?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/563?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113297?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113297?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/563?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113297?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111629?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111629?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114168?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112541?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112541?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111629?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112541?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111839?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111839?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/834?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/834?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/113257?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/113257?units=mks> (referer: None)\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111839?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/834?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/113257?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111132?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111131?units=mks> (referer: None)\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110404?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112978?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111131?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112713?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/113040?units=mks> (referer: None)\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114168?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114168?units=mks> (referer: None)\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110929?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1572?units=mks> (referer: None)\n",
      "2023-05-07 14:42:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/113040?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114168?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112429?units=mks> (referer: None)\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1871?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111132?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111706?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1917?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1572?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111708?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112429?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110404?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112978?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112713?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1413?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110929?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1871?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116361?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110987?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/894?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111132?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111132?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/894?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111132?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111706?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111708?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1917?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112978?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112978?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110404?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1871?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1871?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110929?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110929?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1413?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112978?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110404?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1871?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110929?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/116361?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114326?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110987?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1939?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1917?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1917?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1917?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111708?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111708?units=mks> (referer: None)\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110746?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100761?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111708?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15005?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112026?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111706?units=mks> (referer: None)\n",
      "2023-05-07 14:42:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111706?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110987?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110987?units=mks> (referer: None)\n",
      "2023-05-07 14:42:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110987?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112713?units=mks> (referer: None)\n",
      "2023-05-07 14:42:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1413?units=mks> (referer: None)\n",
      "2023-05-07 14:42:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112713?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/1413?units=mks>\n",
      "\n",
      "{'Name': 'China PR', 'League': '[World] Friendly International', 'Overall': '68', 'Attack': '69', 'Midfield': '67', 'Defence': '67', 'Home stadium': 'Stadion Olympik', 'Rival team': 'Australia', 'International prestige': '3', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '28.73', 'Whole team average age': '28.70', 'Captain': 'Wu Xi', 'Short free kick': 'Wang Shangyuan', 'Long free kick': 'Wang Shangyuan', 'Left short free kick': 'Wang Shangyuan', 'Right short free kick': 'Wang Shangyuan', 'Penalties': 'Wu Lei', 'Left corner': 'Wu Xi', 'Right corner': 'Wu Xi'}\n",
      "2023-05-07 14:42:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100761?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1939?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/116361?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:17 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/116361?units=mks> (referer: None)\n",
      "2023-05-07 14:42:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/417?units=mks> (referer: None)\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111011?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/116361?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112026?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/417?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/15005?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/165?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101028?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110746?units=mks> (referer: None)\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/1939?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/1939?units=mks> (referer: None)\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/114326?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110746?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/680?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/1939?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100761?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100761?units=mks> (referer: None)\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/681?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1961?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111011?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100761?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/15005?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/15005?units=mks> (referer: None)\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101028?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/165?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/15005?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/682?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/114326?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:18 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/114326?units=mks> (referer: None)\n",
      "2023-05-07 14:42:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/689?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111527?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112026?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1961?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/114326?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/680?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111011?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111011?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://sofifa.com/team/111527?units=mks>\n",
      "\n",
      "{'Name': 'Qatar', 'League': '[World] Friendly International', 'Overall': '68', 'Attack': '71', 'Midfield': '69', 'Defence': '68', 'Home stadium': 'Stadion Hanguk', 'Rival team': 'Australia', 'International prestige': '3', 'Domestic prestige': None, 'Club worth': None, 'Starting XI average age': '28.00', 'Whole team average age': '26.39', 'Captain': 'H. Al Haydos', 'Short free kick': 'B. Al Rawi', 'Long free kick': 'A. Hatem', 'Left short free kick': 'B. Al Rawi', 'Right short free kick': 'B. Al Rawi', 'Penalties': 'A. Afif', 'Left corner': 'A. Afif', 'Right corner': 'A. Hatem'}\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/681?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112026?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/693?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111011?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101028?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101028?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/165?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/165?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/682?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/689?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101028?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/165?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/695?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/694?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/698?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112828?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/681?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/681?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/680?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/680?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/681?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/680?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/710?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/693?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111042?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/110770?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/695?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/110770?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/689?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/689?units=mks> (referer: None)\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/694?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/698?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/689?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1961?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/203?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112828?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111042?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/210?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1961?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/693?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/693?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/710?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/982?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/695?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/695?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/693?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/695?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/698?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/698?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/682?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110816?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112828?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112828?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/698?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/682?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/203?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112828?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111042?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111042?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/210?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/710?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/710?units=mks> (referer: None)\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111042?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/982?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100081?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/710?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101112?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110588?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/254?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/111326?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110816?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1797?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/203?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/203?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/256?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/111326?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/203?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/210?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/210?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100081?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/982?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/982?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/210?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/982?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/101112?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/694?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/254?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/110588?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110816?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110816?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/694?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112393?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110816?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1807?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/256?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100632?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100081?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100081?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/543?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100081?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/101112?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/101112?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111139?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1797?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112392?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/254?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/254?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112393?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/101112?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1797?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/110588?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/110588?units=mks> (referer: None)\n",
      "2023-05-07 14:42:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111397?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112392?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/254?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/256?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/256?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/100632?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/110588?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112168?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/543?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/256?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10032?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111139?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/820?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/308?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/111397?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112184?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1807?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112168?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1807?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/100632?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/100632?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1854?units=mks> (failed 1 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/543?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/543?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/100632?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/10032?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111139?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111139?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/543?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/310?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/308?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/111397?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/111397?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111139?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/310?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/820?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112184?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/111397?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112393?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/1854?units=mks> (failed 2 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112168?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112168?units=mks> (referer: None)\n",
      "2023-05-07 14:42:22 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112184?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:22 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/112184?units=mks> (referer: None)\n",
      "2023-05-07 14:42:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/10032?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/10032?units=mks> (referer: None)\n",
      "2023-05-07 14:42:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/308?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/308?units=mks> (referer: None)\n",
      "2023-05-07 14:42:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112393?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:42:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112168?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/112184?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/820?units=mks> (failed 3 times): 429 Unknown Status\n",
      "2023-05-07 14:42:23 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://sofifa.com/team/820?units=mks> (referer: None)\n",
      "2023-05-07 14:42:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/10032?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/308?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://sofifa.com/team/820?units=mks>: HTTP status code is not handled or not allowed\n",
      "2023-05-07 14:42:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/1854?units=mks> (referer: None)\n",
      "2023-05-07 14:42:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/1854?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:43:10 [scrapy.extensions.logstats] INFO: Crawled 709 pages (at 109 pages/min), scraped 25 items (at 2 items/min)\n",
      "2023-05-07 14:44:10 [scrapy.extensions.logstats] INFO: Crawled 709 pages (at 0 pages/min), scraped 25 items (at 0 items/min)\n",
      "2023-05-07 14:44:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/3?units=mks> (failed 3 times): User timeout caused connection failure: Getting https://sofifa.com/team/3?units=mks took longer than 180.0 seconds..\n",
      "2023-05-07 14:44:23 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://sofifa.com/team/112134?units=mks> (failed 3 times): User timeout caused connection failure: Getting https://sofifa.com/team/112134?units=mks took longer than 180.0 seconds..\n",
      "2023-05-07 14:44:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://sofifa.com/team/3?units=mks>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1693, in _inlineCallbacks\n",
      "    result = context.run(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\twisted\\python\\failure.py\", line 518, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 52, in process_request\n",
      "    return (yield download_func(request=request, spider=spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 892, in _runCallbacks\n",
      "    current.result = callback(  # type: ignore[misc]\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 398, in _cb_timeout\n",
      "    raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n",
      "twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://sofifa.com/team/3?units=mks took longer than 180.0 seconds..\n",
      "2023-05-07 14:44:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://sofifa.com/team/112134?units=mks>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1693, in _inlineCallbacks\n",
      "    result = context.run(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\twisted\\python\\failure.py\", line 518, in throwExceptionIntoGenerator\n",
      "    return g.throw(self.type, self.value, self.tb)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 52, in process_request\n",
      "    return (yield download_func(request=request, spider=spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 892, in _runCallbacks\n",
      "    current.result = callback(  # type: ignore[misc]\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 398, in _cb_timeout\n",
      "    raise TimeoutError(f\"Getting {url} took longer than {timeout} seconds.\")\n",
      "twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://sofifa.com/team/112134?units=mks took longer than 180.0 seconds..\n",
      "2023-05-07 14:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sofifa.com/team/112707?units=mks> (failed 1 times): User timeout caused connection failure: Getting https://sofifa.com/team/112707?units=mks took longer than 180.0 seconds..\n",
      "2023-05-07 14:44:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sofifa.com/team/112707?units=mks> (referer: None)\n",
      "2023-05-07 14:44:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sofifa.com/team/112707?units=mks> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 257, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\utils\\python.py\", line 312, in __next__\n",
      "    return next(self.data)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 353, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 104, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\\spiders\\collect_teams_info.py\", line 54, in parse\n",
      "    team_info[\"Club worth\"] = response.xpath('//li[contains(label, \"Club worth\")]/text()').re_first(r'€([\\d\\.\\d\\])([BM]')\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 230, in re_first\n",
      "    for el in iflatten(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 27, in iflatten\n",
      "    for el in x:\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 231, in <genexpr>\n",
      "    x.re(regex, replace_entities=replace_entities) for x in self\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 476, in re\n",
      "    return extract_regex(\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\site-packages\\parsel\\utils.py\", line 67, in extract_regex\n",
      "    regex = re.compile(regex, re.UNICODE)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 252, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_compile.py\", line 764, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 948, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 443, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "  File \"d:\\users\\asus\\anaconda3\\lib\\sre_parse.py\", line 836, in _parse\n",
      "    raise source.error(\"missing ), unterminated subpattern\",\n",
      "re.error: missing ), unterminated subpattern at position 1\n",
      "2023-05-07 14:44:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-05-07 14:44:47 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: dataset/teams_info.json\n",
      "2023-05-07 14:44:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 3,\n",
      " 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,\n",
      " 'downloader/request_bytes': 680226,\n",
      " 'downloader/request_count': 1786,\n",
      " 'downloader/request_method_count/GET': 1786,\n",
      " 'downloader/response_bytes': 16699320,\n",
      " 'downloader/response_count': 1783,\n",
      " 'downloader/response_status_count/200': 209,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'downloader/response_status_count/429': 1573,\n",
      " 'elapsed_time_seconds': 216.899121,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 5, 7, 7, 44, 47, 416490),\n",
      " 'httpcompression/response_bytes': 25140322,\n",
      " 'httpcompression/response_count': 210,\n",
      " 'httperror/response_ignored_count': 500,\n",
      " 'httperror/response_ignored_status_count/429': 500,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/DEBUG': 1819,\n",
      " 'log_count/ERROR': 688,\n",
      " 'log_count/INFO': 514,\n",
      " 'response_received_count': 710,\n",
      " 'retry/count': 1074,\n",
      " 'retry/max_reached': 502,\n",
      " 'retry/reason_count/429 Unknown Status': 1073,\n",
      " 'retry/reason_count/twisted.internet.error.TimeoutError': 1,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/403': 1,\n",
      " 'scheduler/dequeued': 1785,\n",
      " 'scheduler/dequeued/memory': 1785,\n",
      " 'scheduler/enqueued': 1785,\n",
      " 'scheduler/enqueued/memory': 1785,\n",
      " 'spider_exceptions/error': 184,\n",
      " 'start_time': datetime.datetime(2023, 5, 7, 7, 41, 10, 517369)}\n",
      "2023-05-07 14:44:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# remove data collect if exist\n",
    "#!rm /content/fifa_crawler/fifa_crawler/dataset/teams_info.json\n",
    "\n",
    "# Local notebook\n",
    "!rm /dataset/teams_info.json\n",
    "\n",
    "!scrapy crawl teams_info -o dataset/teams_info.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMnag5Vc1Ov_"
   },
   "source": [
    "<b>Đọc thông tin collect được lưu vào biến df</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is A000-224A\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\fifa_crawler\\fifa_crawler\n",
      "\n",
      "03/16/2023  12:43 AM    <DIR>          .\n",
      "03/13/2023  10:58 PM    <DIR>          ..\n",
      "03/11/2023  11:15 PM                 0 __init__.py\n",
      "03/16/2023  12:43 AM    <DIR>          __pycache__\n",
      "05/07/2023  02:41 PM    <DIR>          dataset\n",
      "03/13/2023  10:58 PM               279 items.py\n",
      "03/13/2023  10:58 PM             3,761 middlewares.py\n",
      "03/13/2023  10:58 PM               378 pipelines.py\n",
      "03/16/2023  12:43 AM             3,745 settings.py\n",
      "05/07/2023  02:37 PM    <DIR>          spiders\n",
      "               5 File(s)          8,163 bytes\n",
      "               5 Dir(s)   9,021,599,744 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9C7zaCd8H6l",
    "outputId": "813efafb-f748-4fc0-9158-99110cd24d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bảng gồm  21  thuộc tính (trường dữ liệu) và  451  dòng (records).\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_json('dataset/teams_info.json', encoding='utf-8-sig')\n",
    "# Google colab\n",
    "#file_path_teams_info_json = '/content/fifa_crawler/fifa_crawler/dataset/teams_info.json'\n",
    "\n",
    "# Local notebook\n",
    "file_path_teams_info_json = 'dataset/teams_info.json'\n",
    "\n",
    "df = pd.read_json(file_path_teams_info_json, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Bảng gồm \",df.shape[1],\" thuộc tính (trường dữ liệu) và \",len(df),\" dòng (records).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "uS2s5f96888e",
    "outputId": "044b1903-a7e2-4ec6-d948-617943c6e3e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>League</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Midfield</th>\n",
       "      <th>Defence</th>\n",
       "      <th>Home stadium</th>\n",
       "      <th>Rival team</th>\n",
       "      <th>International prestige</th>\n",
       "      <th>Domestic prestige</th>\n",
       "      <th>...</th>\n",
       "      <th>Starting XI average age</th>\n",
       "      <th>Whole team average age</th>\n",
       "      <th>Captain</th>\n",
       "      <th>Short free kick</th>\n",
       "      <th>Long free kick</th>\n",
       "      <th>Left short free kick</th>\n",
       "      <th>Right short free kick</th>\n",
       "      <th>Penalties</th>\n",
       "      <th>Left corner</th>\n",
       "      <th>Right corner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Toulouse</td>\n",
       "      <td>[France] Ligue 1</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>75</td>\n",
       "      <td>72</td>\n",
       "      <td>Stadion Europa (Generic Europe 2)</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.27</td>\n",
       "      <td>23.04</td>\n",
       "      <td>B. Dejaegere</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "      <td>B. van den Boomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>SD Eibar</td>\n",
       "      <td>[Spain] La Liga 2</td>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>Municipal de Ipurua</td>\n",
       "      <td>Real Sociedad</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.73</td>\n",
       "      <td>26.62</td>\n",
       "      <td>Arbilla</td>\n",
       "      <td>Arbilla</td>\n",
       "      <td>Arbilla</td>\n",
       "      <td>Arbilla</td>\n",
       "      <td>Arbilla</td>\n",
       "      <td>Corpas</td>\n",
       "      <td>Corpas</td>\n",
       "      <td>Corpas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Botafogo</td>\n",
       "      <td>1014</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>None</td>\n",
       "      <td>Fluminense</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.45</td>\n",
       "      <td>27.80</td>\n",
       "      <td>Enaldo Toxeto</td>\n",
       "      <td>José Mirazar</td>\n",
       "      <td>José Mirazar</td>\n",
       "      <td>José Mirazar</td>\n",
       "      <td>Enaldo Toxeto</td>\n",
       "      <td>Paulão Junqueiras</td>\n",
       "      <td>Edercinho Sepa</td>\n",
       "      <td>Enaldo Toxeto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Independiente del Valle</td>\n",
       "      <td>[Ecuador] Liga Pro</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>Stade Municipal</td>\n",
       "      <td>LDU Quito</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.45</td>\n",
       "      <td>25.20</td>\n",
       "      <td>C. Pellerano</td>\n",
       "      <td>J. Sornoza</td>\n",
       "      <td>J. Sornoza</td>\n",
       "      <td>J. Sornoza</td>\n",
       "      <td>J. Sornoza</td>\n",
       "      <td>L. Faravelli</td>\n",
       "      <td>J. Sornoza</td>\n",
       "      <td>J. Sornoza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Las Palmas</td>\n",
       "      <td>[Spain] La Liga 2</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>72</td>\n",
       "      <td>Estadio de Gran Canaria</td>\n",
       "      <td>Tenerife</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.09</td>\n",
       "      <td>25.17</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "      <td>Jonathan Viera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name              League  Overall  Attack  Midfield  \\\n",
       "446                 Toulouse    [France] Ligue 1       72      71        75   \n",
       "447                 SD Eibar   [Spain] La Liga 2       72      70        71   \n",
       "448                 Botafogo                1014       71      71        71   \n",
       "449  Independiente del Valle  [Ecuador] Liga Pro       71      70        73   \n",
       "450               Las Palmas   [Spain] La Liga 2       72      72        73   \n",
       "\n",
       "     Defence                       Home stadium     Rival team  \\\n",
       "446       72  Stadion Europa (Generic Europe 2)       Bordeaux   \n",
       "447       72                Municipal de Ipurua  Real Sociedad   \n",
       "448       71                               None     Fluminense   \n",
       "449       70                    Stade Municipal      LDU Quito   \n",
       "450       72            Estadio de Gran Canaria       Tenerife   \n",
       "\n",
       "     International prestige  Domestic prestige  ... Starting XI average age  \\\n",
       "446                       1                2.0  ...                   25.27   \n",
       "447                       1                8.0  ...                   26.73   \n",
       "448                       4                6.0  ...                   27.45   \n",
       "449                       5                8.0  ...                   28.45   \n",
       "450                       1                8.0  ...                   25.09   \n",
       "\n",
       "     Whole team average age         Captain    Short free kick  \\\n",
       "446                   23.04    B. Dejaegere  B. van den Boomen   \n",
       "447                   26.62         Arbilla            Arbilla   \n",
       "448                   27.80   Enaldo Toxeto       José Mirazar   \n",
       "449                   25.20    C. Pellerano         J. Sornoza   \n",
       "450                   25.17  Jonathan Viera     Jonathan Viera   \n",
       "\n",
       "        Long free kick Left short free kick Right short free kick  \\\n",
       "446  B. van den Boomen    B. van den Boomen     B. van den Boomen   \n",
       "447            Arbilla              Arbilla               Arbilla   \n",
       "448       José Mirazar         José Mirazar         Enaldo Toxeto   \n",
       "449         J. Sornoza           J. Sornoza            J. Sornoza   \n",
       "450     Jonathan Viera       Jonathan Viera        Jonathan Viera   \n",
       "\n",
       "             Penalties        Left corner       Right corner  \n",
       "446  B. van den Boomen  B. van den Boomen  B. van den Boomen  \n",
       "447             Corpas             Corpas             Corpas  \n",
       "448  Paulão Junqueiras     Edercinho Sepa      Enaldo Toxeto  \n",
       "449       L. Faravelli         J. Sornoza         J. Sornoza  \n",
       "450     Jonathan Viera     Jonathan Viera     Jonathan Viera  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVXSFBgI9cyb"
   },
   "source": [
    "## B. Khám phá dữ liệu (thường đan xen với tiền xử lý dữ liệu)\n",
    "\n",
    "1. Mỗi dòng có ý nghĩa gì? Có vấn đề các dòng có ý nghĩa khác nhau không?\n",
    "\n",
    "* Mỗi dòng trong bảng này tương ứng với thông tin về một đội bóng.  \n",
    "\n",
    "*  Nếu mỗi đội bóng trong bảng đều có thông tin khác nhau, điều này có ý nghĩa rất quan trọng. \n",
    "\n",
    "  > Bảng này chứa thông tin chi tiết về các đội bóng, bao gồm điểm số, ngân sách, độ tuổi, cầu thủ xuất sắc, sân nhà và nhiều yếu tố khác. \n",
    "  \n",
    "  > Các thông tin này là **quan trọng trong việc phân tích và so sánh hiệu suất của các đội bóng**, đưa ra dự đoán và quyết định về các cược và cách chơi trong các trận đấu. \n",
    "  \n",
    "  >Nếu tất cả các đội bóng trong bảng đều có thông tin giống nhau, thì bảng sẽ không có giá trị trong việc phân tích và so sánh hiệu suất của các đội bóng đó. \n",
    "\n",
    "Tuy nhiên, nếu mỗi đội bóng trong bảng đều có thông tin khác nhau, thì bảng sẽ cung cấp cho chúng ta **một tài nguyên quý giá** trong việc **tìm hiểu và đưa ra các quyết định về các đội bóng trong các giải đấu bóng đá**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVXDTsadDjj2"
   },
   "source": [
    "2.Mỗi cột có ý nghĩa gì?\n",
    "\n",
    "- Các cột trong dữ liệu này có ý nghĩa như sau:\n",
    "\n",
    "| Field | Description |\n",
    "| --- | --- |\n",
    "| Name | Tên đội bóng. |\n",
    "| League | Tên giải đấu mà đội bóng đang tham gia. |\n",
    "| Overall | Điểm số tổng thể của đội bóng. Khoảng biểu diễn [1, 100].|\n",
    "| Attack | Điểm số tấn công của đội bóng. Khoảng biểu diễn [1, 10]. |\n",
    "| Midfield | Điểm số tiền vệ của đội bóng. Khoảng biểu diễn [1, 100]. |\n",
    "| Defence | Điểm số phòng thủ của đội bóng. Khoảng biểu diễn [1, 100]. |\n",
    "| Home stadium | Sân nhà của đội bóng. |\n",
    "| Rival team | Đối thủ chính của đội bóng. |\n",
    "| International prestige | Độ uy tín của đội bóng trong các giải đấu quốc tế. Khoảng biểu diễn [1, 10]. |\n",
    "| Domestic prestige | Độ uy tín của đội bóng trong các giải đấu trong nước. Khoảng biểu diễn [1, 10]. |\n",
    "| Transfer budget | Ngân sách chuyển nhượng của đội bóng. Khoảng biểu diễn [€100.0M, €10.0B]. |\n",
    "| Starting XI average age | Tuổi trung bình của đội hình xuất phát. Khoảng biểu diễn [18, 30]. |\n",
    "| Whole team average age | Tuổi trung bình của toàn đội bóng. Khoảng biểu diễn [18, 30].  |\n",
    "| Captain | Tên cầu thủ đội trưởng của đội bóng. |\n",
    "| Short free kick | Cầu thủ thực hiện đá phạt ngắn. |\n",
    "| Long free kick | Cầu thủ thực hiện đá phạt dài. |\n",
    "| Left short free kick | Cầu thủ thực hiện đá phạt ngắn bên trái. |\n",
    "| Right short free kick | Cầu thủ thực hiện đá phạt ngắn bên phải. |\n",
    "| Penalties | Cầu thủ thực hiện đá phạt đền. |\n",
    "| Left corner | Cầu thủ thực hiện đá phạt góc bên trái. |\n",
    "| Right corner | Cầu thủ thực hiện đá phạt góc bên phải. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dnQ5vatDoCG"
   },
   "source": [
    "3. Mỗi cột hiện đang có kiểu dữ liệu gì? Khoảng biểu diễn của kiểu dữ liệu đó ra sao?\n",
    "Có cột nào có kiểu dữ liệu chưa phù hợp để có thể xử lý tiếp hay không?\n",
    "\n",
    "* Sau khi tạo DataFrame, chúng ta có thể sử dụng phương thức info() để xem thông tin tổng quan về dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTWpzexCDvmC",
    "outputId": "26e83109-500f-4fc1-bb0e-6c989a24c038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 451 entries, 0 to 450\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Name                     451 non-null    object \n",
      " 1   League                   451 non-null    object \n",
      " 2   Overall                  451 non-null    int64  \n",
      " 3   Attack                   451 non-null    int64  \n",
      " 4   Midfield                 451 non-null    int64  \n",
      " 5   Defence                  451 non-null    int64  \n",
      " 6   Home stadium             436 non-null    object \n",
      " 7   Rival team               451 non-null    object \n",
      " 8   International prestige   451 non-null    int64  \n",
      " 9   Domestic prestige        420 non-null    float64\n",
      " 10  Club worth               420 non-null    object \n",
      " 11  Starting XI average age  451 non-null    float64\n",
      " 12  Whole team average age   451 non-null    float64\n",
      " 13  Captain                  451 non-null    object \n",
      " 14  Short free kick          451 non-null    object \n",
      " 15  Long free kick           451 non-null    object \n",
      " 16  Left short free kick     451 non-null    object \n",
      " 17  Right short free kick    451 non-null    object \n",
      " 18  Penalties                449 non-null    object \n",
      " 19  Left corner              451 non-null    object \n",
      " 20  Right corner             451 non-null    object \n",
      "dtypes: float64(3), int64(5), object(13)\n",
      "memory usage: 74.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jqd7EyjG_DL"
   },
   "source": [
    "Qua đó ta có thể thấy:\n",
    "\n",
    "* Dữ liệu có 712 dòng và 21 cột.\n",
    "\n",
    "* Các cột chứa các kiểu dữ liệu khác nhau: int64, float64, và object..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_2kCKiQD1HS"
   },
   "source": [
    "<b>Kết luận:</b> Các một số cột có kiểu dữ liệu không phù hợp để xử lý tiếp. Tuy nhiên, có thể cần chuyển đổi kiểu dữ liệu của một số cột để phù hợp hơn với các phép tính và xử lý dữ liệu, Ví dụ như chuyển đổi kiểu dữ liệu của cột \"Overall\" từ chuỗi sang số thực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "he1bOBdPFlNH"
   },
   "outputs": [],
   "source": [
    "# Chuyển kiểu dữ liệu của các cột số thành kiểu số liên tục\n",
    "df['Overall'] = pd.to_numeric(df['Overall'], errors='coerce')\n",
    "df['Attack'] = pd.to_numeric(df['Attack'], errors='coerce')\n",
    "df['Defence'] = pd.to_numeric(df['Defence'], errors='coerce')\n",
    "df['Midfield'] = pd.to_numeric(df['Midfield'], errors='coerce')\n",
    "df['International prestige'] =  pd.to_numeric(df['International prestige'], errors='coerce')\n",
    "df['Domestic prestige'] = pd.to_numeric(df['Domestic prestige'], errors='coerce')\n",
    "df['Whole team average age'] = pd.to_numeric(df['Whole team average age'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXE1gAbVF5nZ"
   },
   "source": [
    "* Với mỗi cột, dữ liệu phân bố như thế nào? Để phân tích phân bố dữ liệu của từng cột, ta cần biểu diễn các giá trị trong cột dưới dạng biểu đồ phù hợp. Một số cách để thực hiện phân tích phân bố dữ liệu bao gồm sử dụng biểu đồ tần suất (histogram), biểu đồ hộp (box plot), hoặc biểu đồ phân phối xác suất (probability plot)... Ta cần chuyển đổi kiểu dữ liệu của các cột chứa dữ liệu số (int64 hoặc float64) sang kiểu dữ liệu số liên tục, sau đó sử dụng thư viện matplotlib để vẽ histogram. Với các cột chứa dữ liệu không phải số, ta có thể dùng biểu đồ bar chart để biểu diễn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "tJZklabBE-HM",
    "outputId": "49aa6035-39a9-4a8e-e5e8-b813369cdc99"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Overall value:  54\n",
      "Maximum Overall value:  85\n",
      "The most frequent range of values in the Overall column is: (60, 70]\n",
      "Most frequent value of Overall column: 69\n",
      "Number of occurrences of most frequent value in Overall column: 62\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Overall'\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, 101, 10))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "5V9LARXZJAiU",
    "outputId": "606fbd39-7e08-46e6-b8e5-029b7d90f614"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Attack value:  50\n",
      "Maximum Attack value:  87\n",
      "The most frequent range of values in the Attack column is: (60, 70]\n",
      "Most frequent value of Attack column: 71\n",
      "Number of occurrences of most frequent value in Attack column: 39\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Attack'\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, 101, 10))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "OaloOpHAJW7v",
    "outputId": "3bf11452-e44d-4d62-97d7-6ca0c7167db6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Midfield value:  54\n",
      "Maximum Midfield value:  87\n",
      "The most frequent range of values in the Midfield column is: (64, 72]\n",
      "Most frequent value of Midfield column: 69\n",
      "Number of occurrences of most frequent value in Midfield column: 43\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Midfield'\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, int(value_max + value_min), int(value_max / 10) ))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "4RfL8LM0JdZU",
    "outputId": "8e0e7f96-0687-4d47-b1a8-0d15f507f86c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Defence value:  54\n",
      "Maximum Defence value:  84\n",
      "The most frequent range of values in the Defence column is: (64, 72]\n",
      "Most frequent value of Defence column: 68\n",
      "Number of occurrences of most frequent value in Defence column: 45\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Defence'\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, int(value_max + value_min), int(value_max / 10) ))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "SxgrUkkrJwCM",
    "outputId": "8715fe25-f69e-43bf-fe9a-4cdf773ad5e0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum International prestige value:  1\n",
      "Maximum International prestige value:  10\n",
      "The most frequent range of values in the International prestige column is: (0, 1]\n",
      "Most frequent value of International prestige column: 1\n",
      "Number of occurrences of most frequent value in International prestige column: 190\n"
     ]
    }
   ],
   "source": [
    "column_name = 'International prestige'\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, int(value_max + value_min), int(value_max / 10) ))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 25, 25))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "Q8TrT4YgJ7iQ",
    "outputId": "e1c17dfd-f6f6-4b00-8566-6991828857da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "D:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Domestic prestige value:  1.0\n",
      "Maximum Domestic prestige value:  10.0\n",
      "The most frequent range of values in the Domestic prestige column is: (7, 8]\n",
      "Most frequent value of Domestic prestige column: 8.0\n",
      "Number of occurrences of most frequent value in Domestic prestige column: 50\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Domestic prestige'\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, int(value_max + value_min), int(value_max / 10) ))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "NMRA3vIEKCng",
    "outputId": "0e8ba88e-72bb-44b7-d4d9-f3f7aa58c322"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Whole team average age value:  19.86\n",
      "Maximum Whole team average age value:  29.8\n",
      "The most frequent range of values in the Whole team average age column is: (20, 30]\n",
      "Most frequent value of Whole team average age column: 25.07\n",
      "Number of occurrences of most frequent value in Whole team average age column: 8\n"
     ]
    }
   ],
   "source": [
    "column_name = \"Whole team average age\"\n",
    "\n",
    "plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(column_name + \" ratings\")\n",
    "plt.ylabel(\"Number of teams\")\n",
    "plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "\n",
    "# Divide the column into bins of width 10\n",
    "bins = pd.cut(df[column_name], bins=range(0, 101, 10))\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "bin_counts = bins.value_counts()\n",
    "\n",
    "# Find the bin with the highest count\n",
    "most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s_016O1InOV"
   },
   "source": [
    "Ví dụ về cách vẽ biểu đồ bar chart cho các cột không phải số của dataframe:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "owAaDIa8IleX",
    "outputId": "62c72715-e6de-4748-dab1-e5bd6fd081a3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAD3CAYAAAB/0w9iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU1f3/8dc7K1uIuKGiEkUUUBRFEVSU8uvX2mI3l1prLXaxtaX6rVXbtG7RaqWt/drWDVvbuu9aW6UutS3uCCpKZFOBiAgioGzKkmQ+vz/OiQwxyySZ5E7C5/l4zCOTu5zzmTt37ueeczeZGc4555xrXl7SATjnnHOdhSdN55xzLkOeNJ1zzrkMedJ0zjnnMuRJ0znnnMuQJ03nnHMuQ1lJmpImSbowS2XtLmmdpPz4/xRJ38lG2bG8RySNz1Z5Laj3MkkrJL3b0XV3BpJGS5qXdBzpJH1Z0ttxfTww6XhaK9u/oc5CkknaK76/SdJl7VjXFtutrkxSWVy2Be0xfa5rNmlKqpK0XtJaSaskPSfpDEkfz2tmZ5jZLzIs69NNTWNmi8ysl5nVZvYRmqyvQtJt9cr/rJnd3NayWxjHbsA5wBAz26mB8WMkLe7ImJKWvkEDMLOnzWyfJGNqwJXAD+P6OKNuYNoGsu5lkj5M+390gjEnRtKukm6XtDIuj2mSjk06rvZQf1uWze1WZ5fJdr4d61ZsoLwjaXXcYdw3bfy2kv4W18+3JH2tpXVk2tL8vJmVAP2BicBPgT+3tLLmdJU9kQb0B1aa2XtJB9IRutD32B+YVX9g2gayl5n1ioMPSBv2dMeGmTxJ2wLPAJuAfYHtgauAOySd0A71tds61oXW363RicC3gNHAtsDzwK1p468lrKN9gVOA69OTakbMrMkXUAV8ut6wEUAK2C/+fxNwWXy/PfAwsAp4H3iakJxvjfOsB9YBPwHKAAO+DSwCnkobVhDLmwJcAUwDVgN/B7aN48YAixuKFzgmLpzqWN+raeV9J77PAy4A3gLeA24BSuO4ujjGx9hWAOc3sZxK4/zLY3kXxPI/HT9zKsZxUwPzfuJzpI3bBbg/lrsQOKve9/B8XNZLgWuAorTxRwPz4nK7Dngy7bNXALelTVt/uZcSdoyWAu8AlwH5jcRYAdwH3AasAb7TVGzxezbgw7hMTqq/DOL3eC4wM8Z/N9AtbfxPYrlLYn0G7BXHfQ6YDayNsZ/bSNwNfv9AcYyrLsb5zfxG0usuJrRQFwHLgElA9ziuD+G3sRz4IL7fNa2cKXE5PxfrfwjYDrg9LtfpQFmcVoSk9F5cPjOJv8cG4psC/AJ4Ni6Tx4Ht08bfC7wby3kK2DdtXKbL8hfAa0BeveE/jctXcVlcWW/834EfZ7CuV9CCdayB7+Um4jaqgdhPi8vmKsI26zJgAPAfYCXht387sE2cvqltWUGGy/wbcbmsBC4kbTsbP9eL8XMuA/6viXXvWOCVuAyeA/ZPG3cg8HKs/27gLjZvp08DnmliPR4HzIgxvA1UNLatqFdGU8umwW1pht/jGcAbhN/NtYAaWR4/Be5J+39fYEN835OQE/auF+/Epn7fn6ij2QkaSJpx+CLg+/VXSEKCmwQUxtfoug9Yv6y0hXlL/EDdG1n53gH2i9PcT9zY00TSTPuh3VZv/BQ2J45vAW8CewK9gAeAW+vF9qcY1wHARmBwI8vpFsIGoCTO+zrw7cbirDdvg+MJG/WXgIuAohjnAuAzcfxwYCRQEOucA/wojtuesMIfF8f/L2EHItOk+SBwQ1zmOxJ2Wr7XSPwVsewvxZi7NxVb/R9oQ8sgfo/TCBvSbeP8Z8RxxxA28vsCPQgrfvoPfikwOr7vAxzUSNyNfv8NxdjE95de9++Af8SYSwiJ74o4bjvg+BhzCSFZPVhv3XyTsMEuJSSr1wk7XgWEdeyvcdrPxHVjG0JCGgzs3Eh8U4D5wN7xu5lC2oYiLocSQsL/HfBK2rhMl+VU4JIGhu8Rl88+wJGEDbDSylsfv+Pm1vUK2rCO0XzSrAHOjGV1B/YC/icukx0IOxO/a2y7SMPbrQaXOTCEkFCOiJ/1yvjZ6rZbzwOnxve9gJGNxH0QYafpUCCfkJSqYsxFhKR8NmE7fEKsI9OkOQYYGpf1/oTk/aWGPmsDcTW2bBrclmb4PT5MWNd3J+xUHdNI3f0JOwp7x8/9a+JvjLATsb7e9OcCDzX3G09/teVEoCWEDUN91cDOQH8zq7ZwrMqaKavCzD40s/WNjL/VzF4zsw8Je2VfydIB91MIe3ELzGwd8DPgq/W6Zy4xs/Vm9irwKuEL30KM5STgZ2a21syqgN8Cp7YxvkOAHczsUjPbZGYLCCveVwHM7CUzm2pmNbHOG4Cj4ryfA2aZ2QNmVgP8gZBomiWpL/BZwor7oYVu5avq6m3E82b2oJml4vJqKrZM/cHMlpjZ+4TkMywO/wohecwys4+AS+rNVw0MkdTbzD4ws5cbKT+T7z9jkgScDpxtZu+b2Vrgl2z+vlaa2f1m9lEcdzmfXCZ/NbP5ZrYaeITQyn0ifof3En74dZ+xBBhESEJzzGxpE+H91cxej7+xe9i8LDGzv8T1diMhOR0gqTStnkyW5faEBFvf0rTxTxM2gHXHfE8grDdLaGZdj9pjHauzxMyujmWtN7M3zexfZrbRzJYD/9eKshtb5icQNtTPmNkmwo5C+jayGthL0vZmts7MpjZS/unADWb2gpnVWjhXYyMhAY0kJI3fxe3wfYSeioyY2RQzq4zLeiZwJ61ftnUa3JZm+D1ONLNVZrYI+C9p6289Swnr2TzCDtmJhB0HCDsgq+tNv5rwO8pYW5JmP0JXRn2/IewtPy5pgaTyDMp6uwXj3yKsDNtnFGXTdonlpZddQOjvrpOeaD4iLPj6tmfznl16Wf3aGF9/YJd4AtYqSauAn9fFJ2lvSQ9LelfSGsIGum657ELacos7LpmebNSfsIyXptV7A6HF2ZgtvsNmYstUY8t+i89Wv25Ca+5zwFuSnpQ0qpHyM/n+W2IHQivypbTl9mgcjqQekm6IJyCsIbRetqm3A7gs7f36Bv7vBWBm/yF0Y10LLJP0R0m9m4itwWUpKV/SREnzY0xVcZq67yrTZbmCsLNcX92wFXEdvAs4OQ77GqHbE5pZ16P2WMcaK3tHSXfFE0rWELqF22X9jTt+K9Om/TahpTRX0vQmTqbqD5xTb5ntFsvfBXinXoPlrYYKaYikQyX9V9JySasJ3aNt3eY2tg5m8j1msh0GuJiwA7Yb0I2wQ/0fST0Irfv6v5HehO7rjLUqaUo6hJAQnqk/Lu6xnmNmewKfB34s6f/VjW6kyOZaorulvd+dsCe2gnC8qUdaXPnEDVSG5S4hrHjpZdew5YYqEytiTPXLeqeF5dT3NrDQzLZJe5WY2efi+OuBucBAM+tN2MgojlsK7FpXUGwF7ZpW9hbLDkg/q/dtwh7r9mn19jazpg6Y11/WTcXWVlt8NrZcPzCz6Wb2RUKSf5Cwl9+QbH3/dVYQEtu+acut1DafLHQOoZvy0LhMjozDW7VczOwPZjac0E29N3BeK4r5GvBFQhdwKaF77OOYWrAsnwCOV9pZ9dFXCOvT6/H/O4ETJPUndCveH4c3t65D+65j9cu+Ig7bP5b99XplN7dtaUr932Z3Qtd9KNjsDTM7mbDMfwXcJ6lnA+W8DVxeb5n1MLM7Yx394u++zu5p7+tvO+uf1X8H4TDDbmZWSjjklumybemyyeb3eABwt5ktji3XmwiHAYYQ1sECSQPrTf+Jk/2a0qKkKal33Ou5i3A8rLKBaY6VtFf8stYAtfEFYWO0Z0vqjL4uaUjcW7gUuM/Cqd2vA90kjZNUSDipozhtvmVAWQM/5Dp3AmdL2kNSL8Iezt2xKyxjMZZ7gMsllcQNwo8Je6cZk9Qt/UU4prdG0k8ldY+tgv3iTguEboU1wDpJg4DvpxU3GRgq6Uuxu3ECWybGV4AjFS6fKCV0TdZ9nqWEExd+G7/zPEkDJLWke6ap2KD16wKEZf1NSYPjOnFR3QhJRZJOkVRqZtVsXgcbkpXvv46ZpQhdildJ2jHG00/SZ+IkJYSkukrhbNOLW1NPLPeQ2BooJGwAN9D452xKCWEHaSVhI/rLtDpasiyvIuy1/1nSTnEdPhk4HzivrsVj4dKd5cCNwGNmtirO39y63ljsTa1jbVFCaJmsktSPT+6QtGX9vQ/4vKTDJBURWkMfJwlJX5e0Q1yf6pZPQ8v9T8AZcT2QpJ5xW1hCOC5aA5wlqUDScYQTbuq8CuwraVjc1lTUK7sEeN/MNkgaQdi5ylRLl002v8fpwImS+sbt1qmEXrM3LRzeewC4NC6rwwk7jLc2Ud4nZJo0H5K0lrBncz6hf/+bjUw7kLDXuY7wxV1nZlPiuCuAC2JXwrktiPNWwoH8dwlN7rMA4nGfHxB+gO8QNh7pXZD3xr8rJTV0LOYvseynCGfrbSCcDNAaZ8b6FxBa4HfE8jPVj7BBTX/tQWitD4vxrSB81rrjTecSVua1hB/Q3XWFmdkKQn/+rwkbxCGEM/I2xvH/itPPJJyA8XC9eL5B6HKeTThj7T4a7n5rTKOxRRXAzXFd+EoLysXMHiEco/0v4VDA83HUxvj3VKAqdvWcQWglNCSb33+dn8aYpsb6nyC0LiGcZNOd8D1OJXTdtlZvwnL9gM1nYV7ZinJuifO/Q/iu6x8/y2hZmtlKwokt3WI5Kwk7jqeaWf3v/k5Cy/aOtPlraXpdb0hz61hbXEI40WY1YQf0gXrjW7stw8xmEdazuwgtwrWEE3rq1t9jgFmS1gG/B75qZhsaKOdFwnHNawjrwZuEE3yIx0qPi/9/QDjn4oG0eV8nNECeIJyVWr/X8AeE5LKWsFPaWA9DQ1q6bLL5Pf6KsENQd0bx2cDxaTtnPyD8Bt8jrIffj99H+vXXu3+y2M3qzmJzXVxsbS8GTjGz/yYdTzZJGky43KG4ta1E55ISezlWEbonF7ZjPTcRzlC/oL3q2Br4vWe7MEmfkbSNpGI2Hydo7Ey8TkXhFndFkvoQ9i4f8oTpOgtJn1c4MawnoYegks0nYbkc5kmzaxtFuFZsBaHr60vW+GU9nc33CMfG5hOO92TzeJZz7e2LhBPRlhAOaX3VvNuvU/DuWeeccy5D3tJ0zjnnMuRJ0znnnMuQJ03nnHMuQ540nXPOuQx50nTOOecy5EnTOeecy5AnTeeccy5DnjSdc865DHnSdM455zLkSdM555zLkCdN55xzLkOeNJ1zzrkMedJ0zjnnMuRJ0znnnMuQJ03nnHMuQ540nXPOuQx50nTOOecy5EnTOeecy5AnTeeccy5DnjSdc865DHnSdM61iaQySeslvRL/v0rSj9LGPybpxrT/fyvpxy0o/yZJJzQwfIykh+P7kyS9Wfd/velOk7Rc0ivxdUsD0xws6Q+ZxtRMvKdJuia+r5B0bgvmbfCz1ptmjKTD2hpnhvH8vN7/z7VDHc1+5lziSdM5lw3zzWxYfP8ccBiApDxge2DftGkPA57NpFBJ+ZlMZ2Z3A99pYpK7zWxYfH2jXh0FZvaimZ2VSV05YAxx+WZKUkEr69oiaZpZhyTrXOZJ0zmXbc+yeaO+L/AasFZSH0nFwGBghqT/J2mGpEpJf4njkFQl6SJJzwAnphcs6RhJc+O441obYGwB/lHS48At9VqtPWM802N8X4zDT5P0gKRHJb0h6ddp5X1T0uuSngQOb6C+AZJeTvt/oKSXmomxStIlkl6Oy2iQpDLgDODs2GoeLWkHSffHeKdLOryRz1gRP9cUSQsknZVW14OSXpI0S9J347CJQPdYz+1x2Lr4V5J+I+m1GNtJcfiYWP598Xu6XZLiuItifK/FuNTS7y0XtHbvwznXAmXlk4uAPYHd4msXYCegL9AH6A50q/cSsD6+Pkp7rQKWAIvj6534d0nVxHG1HfahGmFmSyTVSNqdkDyfB/oBo4DVwEzCDvtNwP8zs9djl+n3gd/FYjaY2REQEmX82w34EzAWeBO4uwVhnSTpiPj+9/HvcOAIM1svaUzatOcD/zGzb0naBpgm6Yk4bhhwILARmCfpaqAGuCSWtxr4LzCj3jKZL2m1pGFm9grwzfj5m7PCzA6S9APgXDP7jqRJwDozuxJA0h3AVWb2TFzmjxF2TOp/xgpgEPApoCTGf72ZVQPfMrP3JXUHpku638zKJf0wrQch3XFxWRxA6EmYLumpOO5Aws7SEsIO1OHAM8A1ZnZpjPlW4FjgoQyWQU7xpOlcFpWVTxYwABgaX/vF10Da//e2sax88hxCUvr4VTVx3LJ2rrchda3Nw4D/IyTNwwhJ5TlgH2Chmb0ep78ZmMDmpNlQQhwU53kDQNJtwHczjOduM/th3T8xgfzDzNY3MO3RwBfSjkV2A3aP7/9tZqtjGbOB/oSkMcXMlsfhdwN7N1DujcA3FY7nngSMyCDuB+Lfl2i8Zf1pYEhaw623pJL4vv5nnGxmG4GNkt4j7LQtBs6S9OU4zW6E9XVlE3EdAdxpZrXAstjCPgRYA0wzs8UACse5ywhJ81OSfgL0ALYFZuFJ07mtS1n55DzC3vZR8TUa2C6hcIoJe/9btAzKyicvBqbE13+rJo5b0AGx1B3XHEronn0bOIewUf0LoRXdlA8bGW7ZCrCJOgQcb2bzthgoHUpoYdapZfM2NJO47gcuBv4DvGRmTSWlOnX1pddVXx4wqv4OQEyi9T/jJ+KPrexPxzI+kjSFsKPQlKa+v4bq6AZcBxxsZm/HnZbm6shJnjSda6Gy8sl9gS8B4whJcptkI2rWrsDX44uy8smLCAn0EeDhqonj1rVDnc8SkuSC2Bp5P3Z17gucDqwDyiTtZWZvAqcCTzZT5lxgD0kDzGw+cHI7xA2he/NMSWeamUk60MxmNDH9C8DvJW1H2Ck4EXi1/kRmtkHSY8D1wLfbEN9aoHfa/48DPwR+A5DWBZypUuCDmDAHASPTxlVLKoxduOmeAr4n6WZCq/FI4DxCb0BD6hLkCkm9gBOA+1oQY87wpOlcBsrKJ/cndI8dR2hBdeaT6HYHvhFfG8rKJz9O2ID9o2riuNVZqqOS0G15R71hvcxsBYSTZ4B7Fc7snA5MaqrAmHS+C0yWtILQ5bdfluJN9wtCN/HMeLJKFeH4W2NxLY0tp+eBpcDLQGNn/d5OWIceb0N8DwH3KZygdCZwFnCtpJmEbfpThJOFMvUocEacfx4wNW3cHwnL4WUzOyVt+N8Ix6hfJbSyf2Jm78ak+wlmtkrSnwjrQBXh++6UZJbN3g7nuo6y8sklwNcIrYJDEg6nI2wC/kU4QeXBqonjajKZKZ7R+bCZtUcCy1jsZjzXzBpNcEmLx0lLzezCpGNxreNJ07l6ysonH0I4weSrQK+Ew0nKUuDPwB+rJo57u6kJJe1GOIa5spEzLdtdvOThYsKxwlOTiKE5kv5GOElsbF1r23U+njSdA8rKJxcSjvmdSThl3gW1hGOf11ZNHPdo0sE4lzRPmm6rVlY+uRvwLeAnhMsHXONeASYC91ZNHJdKOhjnkuBJ022Vyson9yScLHEOsHPC4XQ2c4BLgXs8ebqtjSdNt1UpK5+cT7jk4VJgh4TD6exmAT+pmjjun0kH4lxH8aTpthpl5ZPHEi4lGJp0LF3MI8DZVRPHzWt2Suc6OU+arssrK5+8F/Bb4AtJx9KFVQNXA5dm8VpP53KOJ03XZZWVTy4AfgZcABQlHM7W4j3gx1UTx92edCDOtQdPmq5LKiufPJRwkf5BCYeytXoAOKNq4rjlSQfiXDZ50nRdSmxdlgMX4q3LpC0Hvl81cdz9SQfiXLZ40nRdRln55L0J9zodnnQsbgt3ABOqJo5blXQgzrWVJ03XJZSVT/4q4ebSJc1N6xKxADiuauK4Tzz9w7nOpDM/qcE5qCgtXHTRXpcXUHMznjBz2Z7A82Xlk3PyvrDOZcpbmq7zqijdifBIq8OnpfZ56iubLj4y6ZBcRq4lXNdZ/xmNzuU8b2m6zqmi9EDgJeBwgBF58448PX/yc8kG5TI0AZhSVj7Z78jkOh1vabrOp6J0LPAg9bpjzfjw2E2XL51le+yVTGCuhd4APlM1cdzCpANxLlPe0nSdS0XpVwi3bfvE8UuJng8UXZzfk/VrOz4w1woDgefKyicn8gxO51rDk6brPCpKfwjcSRPXXxarZo/JRT9/reOCcm20E/BkWfnkTyUdiHOZ8KTpOoeK0ssI9zZtdp0ty1s26rKCPz/Z/kG5LOkNPFpWPvn4pANxrjmeNF3uqyi9HDi/JbOckv/vw8fkvTKznSJy2VcE3FlWPtlvqu9ymp8I5HJbRenPgF+2ZtZa07JRG6/Je48+fpZm57ER+ELVxHGPJx2Icw3xlqbLXRWlZ9LKhAmQL+v7WPFP38mntiaLUbn2VQw8WFY++aikA3GuIZ40XW6qKP0m8Pu2FtNH64b9pfA3z2YhItdxugMPl5VPHpV0IM7V50nT5Z6K0uOBPwHKRnFH5c886mv5T0zNRlmuw/QiJE6/5tblFD+m6XJLuNPPM0CPbBZrxpqjN/36/Tds17Jsluva3VxglD8hxeUKb2m63FFRuiPhTj9ZTZgAEr3/UXRBdXc2fpTtsl27GgTcE5+T6lziPGm63FBRWki4+fru7VVFd20a+GDRhTPaq3zXbv4H+F3SQTgHnjRd7rgaGN3eleyTt/jwnxXc8XR71+OybkJZ+eQzkg7COT+m6ZJXUfo9YFJHVWfGppOrz39jamrffTuqTpcVG4ERVRPH+U0rXGI8abpkVZQOITziq1tHVltjeUtGbLy2+H1Kt+vIel2bzQEOrpo4zo9Nu0R496xLTjiOeRsdnDABCpTa5dHi8iqRSnV03a5NBuPHN12CPGm6JF0CHJhU5Ttq9fDrCn/vxzc7n9P95u4uKZ40XTIqSg8HfpJ0GMfkTT/yy3lPT086Dtdifyorn9wv6SDc1seTput4FaUlwK1AftKhSOi3hdfv1V/vLk46FtcifYDrkg7CbX08abokXA7skXQQdfJEn38W/XxtEdUbk47FtcgXysonn5B0EG7r4mfPuo5VUToUmEEOtDLrm5na4+kvbLq83a8VdVm1FBhUNXHcmqQDcVsHb2m6jvYHcjBhAuyft3D0/+bf/0zScbgW2Rm4NOkg3NbDW5qu41SUfgW4O+kwmmLG+uM2XbJohg3cJ+lYXMZqgYP8pgeuI3hL03WMitIewJVJh9Ecie73FF3arTfrVicdi8tYPvCrpINwWwdPmq6j/AzYLekgMlGo2v6PFP9sHng3TCdyTFn55KOSDsJ1fZ40XfurKN0eODvpMFqin1aO+G3hpCeTjsO1yBVJB+C6Pk+ariOcA/RMOoiWOi7v6dHH5L3wctJxuIyNKiuf/IWkg3Bdm58I5NpXRem2QBVQknAkrZIyrThi4++rl7D9zknH4jLyGnBA1cRxfk9h1y68pena24/opAkTIE+2/aPF5SsKqKlOOhaXkf2ALycdhOu6PGm69lNRWgqclXQYbdVbHw29veiXzycdh8vYj5MOwHVdnjRdezoTKE06iGw4NG/ukd/O/+dzScfhMnJYWfnkQ5MOwnVNnjRd+6goLQC+n3QY2XRBwW0HDFHV/KTjcBnpVGdru87Dk6ZrL8cCuyQdRDZJ9Pxb0UV5PVm/NulYXLOOLyufvHvSQbiuJ+OkKalM0npJr6QNq5X0StqrvLWBSFrX2nkbiPO1+H60pNl1/9ebboyk1ZJmSJoj6eJs1N9APTdKGtKG+SsknZvNmDrId5MOoD0Uq2aPh4t+/on1yeWcAuCHSQfhup6WtjTnm9mwtP/Xm9mwtNfEbAbXVmb2NPC5JiZ52swOBA4Gvi5pePpISQVZiOE7ZjY70+kl5eTNzFukorQ/8Jmkw2gve+QtG/WLgr/4jQ9y36ll5ZM7/+/J5ZR26Z6VVCXpEkkvS6qUNCgO30HSv+LwGyS9JWn7evP2kvTvtHm/GIeXxRbhnyTNkvS4pO5x3HBJr0p6HpjQ0njN7EPgJWBAbNn9UdLjwC0x5vslTY+vw2OdFZJujnFUSTpO0q9jzI9KKozTTZF0cHx/tKTn42e7V1KvtOV1kaRngBMzXMbnxXhmSrokbfiDkl6Ky+i7acO/Len1GM+fJF0Th98k6YS06dY1V0cGvkMX7/r/ev4Th4/Je8VvEJ7bdgI+nXQQrmtp64ate73u2ZPSxq0ws4OA64G67sWLgf/E4X8DGjrmsAH4cpzmU8BvJSmOGwhca2b7AquA4+PwvwJnmdmo1nwISdsBI4FZcdBw4Itm9jXg98BVZnZIrO/GtFkHAOOALwK3Af81s6HA+jg8vY7tgQuAT8fP9iJbnhq/wcyOMLO7Moj3aMKyGAEMA4ZLOjKO/paZDSe0ns+StJ2kXYAL42f8H2BQG+toXEVpPvCtZqfr5CQK/lz4mx135IPlScfimnRq0gG4rqWt3Y/r63XXpnsg/n0JOC6+P4J44bGZPSrpgwbmE/DLuIFOAf2AvnHcQjOrO6b6ElAmqRTYxszqustuBT6bYfyjJc2I9Uw0s1mSTgT+YWbr4zSfBoZsztv0llR3sf4jZlYtqZLwpIVH4/BKoKxeXSOBIcCzsawiIP3av5Y8Muvo+JoR/+9FSHBPERJl3cXdu8XhOwFPmtn7AJLuBfZuQx1NGUsXOwGoMfmynR4tLn/l4I3Xb5siz7sBc9OXy8on96qaOC4r50w41+Zjdk3YGP/WptWjRqZNdwqwAzA8JqQqoFu9MuvK7R7LbO29AJ82s2MbGP5h2vs8YFRaEgUgJr6NAGaWklRtm+9JmOKTy1bAv8zs5EZi+bCR4Q0RcIWZ3VAvpjGEJD/KzD6SNIWw7Jpa7jXEHofYoi9qqo4MnND8JF3Htlo77C+Fv37ytOpyf8JGbupB2Gm/JelAXNfQ0cedngG+Ah93//VpYJpS4L2YMD8F9G+qQDNbBayWdEQcdEoW4wV4nLSz8CQ11rJuzlTgcEl7xXJ6SGqutdeYx4BvpR0T7SdpR8Ky+yAmzEGE1i3ANOAoSX3iyU3Hp5VVReiOhtDNXPobqAUAACAASURBVNhMHY0LXbNfauVn6rTG5M886mv5T0xNOg7XqJOan8S5zGT7mGZzZ89eAhwt6WVCF+pSoP41b7cDB0t6kZAA52YQxzeBa+OJQOubm7iFzorxzJQ0GzijNYWY2XLgNOBOSTMJSbTZY4vRBZIW173M7HHgDuD52DV8H+H+ro8CBbH8X8Q6MLN3gF8CLwBPALOBuocs/4mQUKcBhxJbvE3U0ZTDgKYTaxd1ecFfhgzU4qqk43AN+lRZ+eRuzU/mXPMyfsqJpDLgYTPbr9WVScVArZnVSBoFXN/EMdGsyEbcXYGkXma2LrY0/wb8xcz+ltVKKkp/DZyX1TI7kfVW9MZBG2/ot57iHknH4j7hs1UTxz3a/GTONa0lLc1aoFRpNzdohd2B6ZJeBf4AnN6GspolaTTwELCiPevpJCrid/casBB4sB3qaOj48FajuzYNfLDowhnNT+kSkOnJgc41yZ+n6bKjonQ3YFHSYeSC62s+//Svak4enXQcbgtvVE0c19pzCJz7WJe+AN11qCOan2TrcEb+QyMO1eyM7wLlOsTAsvLJeyUdhOv8PGm6bPGkGUkU3170y9I+rHk/6VjcFj6VdACu8/Ok6bLl8KQDyCUFSvV7rLh8gUilko7FfWxE0gG4zs+Tpmu7itLewNCkw8g1O2rVwdcW/uHppONwH/Ok6drMk6bLhlH4utSgz+ZNO/JLec+8mHQcDoB9y8on90w6CNe5+YbOZcNhSQeQqyT0f4XXDdhdyxYnHYsjn813v3KuVTxpumzYqm8c0Zw80eefRT9bU0T1xuandu3Mu2hdm3jSdNmwT9IB5Lpe2jDk3qJLpiUdh/Nj765tPGm6tqkozQP8+rcMHJC3YPRZ+Q88k3QcWzlfV12beNJ0bVUGFCcdRGdxdsF9w4fpzXlJx7EV86Tp2sSTpmsr75ptAYnu9xZd0q2ED1c3P7VrBzuWlU9u7mk9zjXKk6ZrK0+aLVSo2v6PFpfPA7/xc0K8telazZOma6s9kg6gM+qnlSOuLLzhyaTj2Ep50nSt5knTtdX2SQfQWR2f99Toz+RN80eJdbx+SQfgOi9Pmq6tPGm2kkT+9YW/33UXVixNOpatzLZJB+A6L0+arq22SzqAzixPtsMjxeUrCqipTjqWrYgnTddqnjRdW3lLs41K9dHQ24queD7pOLYinjRdq3nSdG3lLc0sGJk358hv5T/yXNJxbCU8abpW86TpWq+itAjolXQYXcWFBbfuP1hvzU86jq2AJ03Xap40XVsUJR1AVyLR68GiC9WT9euSjqWL65F0AK7z8qTpXA4pVs2eDxWdX5l0HF1cftIBuM7Lk6ZzOWbPvHdHXVLw16eSjqML86TpWq0g6QCcc5u9VlT0xi2lJUs2vDOr+vy35j6ypjjVLVW9MEXte31kG3bDN/htZmgNjEs6DNdJedJ0LkEpSD3bvdusW0tL3p/erVtZjTQQGHjm3NonD5tz0+hnR/1iVnXJ8IMAzKo/StW8Mz9VveD9VPWiAkut3gVqywAl+iE6GWHLko7BdV6eNJ3rYJtg46O9es68vXevDXOKigaZ9IkHIx+w0ErzrKb4sKkX7/PsqMtn1hT22F8q7JFfWDY0v7Ds4+kstWF1qubthanqBatSNYuLLbV2d0j5beKaVpN0AK7z8qTp2sI3Phlak6fV95f0mnVfSS8tKijYD+mQxqYtrLENJesZBJCf2tRz1NSL+j932OVzavOLB9efVnndSvOLBg7LLxr48TBLfbgiVb2oqrZ6wbpUzZIe2LoysB3b43N1Ur7eulaT+dOJXFtUlH4EdE86jFz0bn7+u7f3Lnnj4V49e6zIz9sfqTCT+UbMS80494HUgenDNhX2WvncyMtWpfILB7QmFkutWVpbXbUoVb1wQ6pmaS/soz2BPq0pqwuYes7dD49KOgjXOXlL07XVSmDXpIPIFa8XFi68pbRk0b979th+nTQEaaeWlnFUpa2pP6yoet12I1+oqHl+5CVvWV5B/5aWqbzeOxcU778zxft/PCxV+/6iVPXCxanqqupU7bJSbMNebB03q1iRdACu8/Kk6dpqBVtx0jSwad2KZ99S2nvF1O7ddtsk7UkbnzE6ZJE1eGvCbptW9R057ReLnz/0oiUof5e21AGQl7/t7nn52+5Ot+EAmFnKapfPT1UvXFpbU5Wy2uXbYpv2Arq1ta4cs7y5CSSVAXOAeWY2LA6rBdKvof2SmVW1Q3xNxXU78Fngu2Z2X71xU4BzzezFtGEHA98ws7PaMaZBwF+Bg4DzzezK9qorF3jSdG21MukAOlo1VP+7Z4/K23qXrKssLhqYkvbNVtndN9raHhvD8cwGx29YseuI6VcsnHbIz5ejvB2yVS+ApDwV7Dggr2DHAQUcCoBZbbXVLptXW73gvVT1W1jtyh2hZgCde9vxXobTza9LmNH6ev9/TJIIh7tSbY6uCWZ2iqSbWjD9i8CLzU7YNu8DZwFfaud6ckJnXvFdbtgquro+lNY9WNLztXtKSlILCgv2RTqoPeoZMc/mCho9SQig10dL9zj45d+88eJBPylAatfjklJ+oQp22SevYJd96H4EAGY1G1I178xLVS9ckapZlG+1H+wMtXvQeW6W0mxLMxOxNfoI8F9gFPAlSeWE7687cJ+ZXRynrQJuBj4PFAInmtlcSb2Aq4GDAQMuMbP7JR0NXAIUA/OBb5pZi2+vKGkMofV5rKQdgDsID1mYDhwDDCd0yT9sZvvFec4FeplZhaQBwLXADsBHwOlmNje9DjN7D3hP0lZx8asnTddWXTZprsjPW35nScm8v5f0LF6Wnz8UaWR713nka/ZRJtP1Xrto4IGv/H72jGH/W4BU0t5xpZMKuuUX9t83v3DzoVWzjWtT1YsXxEtfCi21eldI7d6RcbXA4lbO113SK/H9QuBsYB9CQvsBgKTzzex9SfnAvyXtb2Yz4zwrzOwgST8AzgW+A1wIrDazoXH+PpK2By4APm1mH0r6KfBj4NJWxl3nYuA/ZnaFpGOA72Ywzx+BM8zsDUmHAtcBY9sYR6fmSdO11aKkA8imqoKCRbeUlix8rGePPmvy8vZDOqIj6x+4xHbOdNo+q98Ysv9rk16dud8ZeyMlegazVFySXzTggPyizSf3Wmr9B6matxbUVi9cZzXvdLPU2v5gLT4xqh20dp3dons2tjTfMrOpadN8RdJ3CdvWnYEhQF3SfCD+fQk4Lr7/NPDVupnN7ANJx8b5ng29vhQB2Xje6hHAl2M9j0r6oKmJYyv4MODeGAeElu9WzZOma6t5SQfQVq8UF829ubT3sqe7d9tlY17eQCCRFlLvD21lcTUDm59ys+1XvnbAkDk3vTh78Gn7I+XUU2eU171PftGg4flFmw/RWmrtstrqt95KVS9Yn6pZ2hP7cA86/pms2dzR+7DujaQ9CC3IQ2Lyu4ktT6LaGP/WsnnbK0K3bDoB/zKzk7MYZ125Dalhy671upjzgFWNHcfdWnnSdG3V6ZJmLdQ+2aN75a29S9bM6Fa8Z204+6/Rk286ymFz7A1Bi7uAd3rvxYNrCrpPfX3gSYcQugVzlvJK+hYU79eX4v0+HpaqXbU4Vb3w7VT1wk2p2mW9sfUDgN7tFEI1sLSdyu5NSKKrJfUlnOU6pZl5Hgd+CPwIQvcsMBW4VtJeZvampB7Armb2ehvjewb4CvCreMy07nj4MmBHSdsB64BjgUfNbI2khZJONLN748lO+5vZq22Mo1PzpOnaaj5hTzWn16UN0vqHe/aYeWfvkpo3igoHm5Rze89HzEpVt3beXZc8PbKmoPuzC/b4wmGk9aV1Bnn52+yal3/grnQL93MwM7PUyoWp6oVLUtULa1O1y7fBNu5Fdp6DufCcux9ulzNczexVSTOAWcAC4NkMZruMkCBfI7RALzGzBySdBtwpqa479AIgk6Q5WVLdevQ84SSeOpfEMk8CniTsPKw1s2pJlwIvEI7Vpp/ocwpwvaQLCCcw3QVskTQVrkV+kbDTkJL0I2CI2SevN+4K/I5Aru0qSl+HlnUrdoRVeXkf3FPSa9b9Jb0KlxTkDyXssees239ds7Cwtm3XeL6555eeWrT7/xyZrZhyhVmq1mqXLaitXvhuqvots9oVO0D1AFr+IPR7zrn74ZOamyger/z4jNJcErt9H65/nWYG8xUDtWZWI2kUcL13vbZcTrcOXKcxjxxJmosL8t+5tXfv+Y/06tH7gwRO5Gmt7Vfb0rYmTIC9Fjx4ZE1BjyeX7HL4UdmIK1dIefkq2HlgXsHOA+l+GABmNRutZuns2uqFK1I1b+VZ7Qd9oWZPmn582itNjEtXC5RKeiWXEku8ucFhQIsSZrQ7cI+kPGATcHo2Y9taeNJ02TCbcBwkEbOKit64ubRkyZQe3fuuz8sbBHS6p3yMnmULCGdbttmg1+84qrqwx5TlOxw4Jhvl5SqpoFiFuw3JK9zt42Fmmz6Mj0/7IFX9doGlVvWDVH82nwSTUdI0s7eB3ZqdsIOZ2SltmPcN4MBmJ3RN8qTpsiEbp8NnLAWp57p3e+2W3iUfTO+++RmUHRlDto2ak93DbENn3Thmxv5nPvnBtoO6VIuzOVJRz/zCPfbPL9zcaA+PT1s0v7Z6wRqrWfpyguG5LsCPabq2qyjdkXAGXrsJz6DsMfOO3iUbZodnUGb1FnJJu3NizZJ8o833k61v+kHnPb22d9nobJfbSS2aMGlsi29271y6znLbK5fLKla/B7yR7WLXSmtu6l3y3Lhdd35+eNlum87fYftDZhUXj+5qCXPXFfZWeyRMgINfvvLwHh8uzeQszq3B1OYnca5p3j3rsuUZstBFuiw/f9ntvUvmPdSrZ88V+XlDkQ7LQmw57cjK1CKgXVpAwvJGvPjLQ6eOqHhhQ/ftDm2POjoRT5quzTxpumx5Fvhma2Z8s7Bw4c2lJW890bPHDvEZlH2zHFtOG/G6tevvMM9SBSOnXXLgcyN/8dKm4tLh7VlXjnsi6QBc5+dJ02XLM5lOaGDTuxXPuaW05L3nu3ffPRvPoOy0zGyn99v/JKY8qy0a9ULF4GdHXfZqTWHPA9q7vhy0eMKksZXNT+Zc0/yYpsuOitXzgLcbG10N1Y/17PHyqTv3fWpY2W7vfnvnvkOe7NFjTEyYW60BS3kzD7bviLryU5t6HDb1oj3yazbM7oj6csyjSQfgugZvabpsehj4ft0/H0kf/r1Xz8q7e5fUzi8s2K+9nkHZmR1VmVpKB14uU1C7ofdhUy/a+dlRl72Ryi/q1JfptNAjSQfgugZPmi6bHlqZl3fiXb1L5v6tpGfRsvz8/TviGZSd2cFvWrfmp8quwpoP+4x6oaL6uZGXVlleQVlH15+Aavx4pssS7551WfPLbfv8e8zu/fIn9Sk9YllBwQikDk8InUleymq3W5PM01WKN63eceS0SwuVqn0nifo72HMTJo3tkjcPdx3Pk6bLmp+fVbUJaXLScXQWgxfZXLXfI7Ca1X3Dyn4jXry8Bku9l1QMHeRvSQfgug5Pmi7b7k46gM5iTKWtSDqGnh8t63/wS79ejdn7ScfSTqqBO5IOwnUdnjRdtj0OrEo6iM7ggAVWknQMAL3XvT3wwFd+9y5d8/mHj0yYNHZ50kG4rsOTpsuqyvGVm4C/Jx1HriuosY2lHzE46Tjq9Fn95pADKq+rwuyjpGPJsluSDsB1LZ40XXvwDVUzhi2wuYLuSceRbrv3Z++/7+y/zsFsY9KxZMn7wENJB+G6Fk+aLusqx1f+B5iRdBy57KhK+yDpGBrSd/lLw/d5/c5XMKtJOpYsuGvCpLGbkg7CdS2eNF17+U3SAeSy/d6y7ZKOoTH9lj576IAFD76AWXYf8tnx/px0AK7r8aTp2su9QFXSQeSibptsXY+NyVyfman+bz9xeP9Fj3XmR4r9d8Kksf7AaZd1njRdu6gcX1kDXJV0HLno4NdtrqAw6TiaM2DhQ6P7vfPUk0nH0Uq/TjoA1zV50nTt6UZgZdJB5JqjXrMPk44hU/u8cfdRO7730pSk42ihmRMmjfUbtLt24UnTtZvK8ZUfAdclHUeu2Wexdarnhe43+y9jtn1/dmdqcV6ZdACu6/Kk6drb1cCGpIPIFb0+sg+Kq9k76ThaatjMa4/qvXrBU0nHkYG3gTuTDsJ1XZ40XbuqHF+5HLgp6Thyxai5Nk+d9Hc3fMZvR/dctyTXTw66csKksV3hchmXozrlj9d1OlcCnf3yhawY/Vqq0143KNCIF385stv6FVOTjqURC4FJSQfhujZPmq7dVY6vnA/cnnQcuWDPd9k16RjaQlj+yGmXHlS0cdWLScfSgAv9ZgauvXnSdB3lPCAn74LTUbZdY8uKatkz6TjaKs9qi0a9ULFvYfW6V5KOJc3LNPM0E0llktZLeiVtWF9Jd0haIOklSc9L+nK7R9sGkgZIekXSugbGffwZJb0q6TlJ+2Sx7k/UmeF8UyQdHN//U9I22Yqpo3nSdB2icnzlMqA86TiSdMRsezPpGLIlP1XdfdTUiwbk16yflXQs0dkTJo21DKabb2bDACQJeBB4ysz2NLPhwFch894ASfmtirYNzOzjz9CI+WY2zMwOAG4Gft5BoWXEzD5nZp32SUieNF1H+hPwXNJBJOXw2alMNuqdRkHtxpLDpl7UL6920+sJh3LfhEljW3Nm71hgk5l9fBzUzN4ys6shJERJv5E0XdJMSd+Lw8dI+q+kO4DK+P+Tku6R9LqkiZJOkTRNUqWkAXG+z0t6QdIMSU9I6huHV0j6S2yNLZB0Vhz+C0n/WxebpMvrxrVAb2IPj6TTJF2TVt7DksbE9+ti+a9KmpoW2x6x9T1d0i/SC5Z0XtqyuSQO6ylpciznNUkn1Q9IUpWk7eP7CyXNlfQvSXdKOjcOPz2W/aqk+yX1aOHnbjeeNF2HqRxfacD3CA8G3ursvpyypGPItsKaj7YZNfWiPkrVLEwohLXAOa2cd19Ct25jvg2sNrNDgEOA0yXtEceNAM43syHx/wOA/wWGAqcCe5vZCMINPs6M0zwDjDSzA4G7gJ+k1TUI+Ews92JJhYR7544HkJRHaAVncm5AXfftfODHwP9lME9PYGpsnT4FnB6H/x64Pi6Dd+smlnQ0MDDGOwwYLulI4BhgiZkdYGb7AY3eZCJ21x4PHAgcBxycNvoBMzskxjOH8F3kBE+arkNVjq98jcx+xF3Kzivt7fxU5z4JqDHF1Wt3GPnCJd2Uql2cQPU/mTBp7KJsFCTp2tiymR4HHQ18Ix4DfQHYjpAoAKaZWfqOwnQzW2rhsWrzCQ9jB6iEj3eWdgUek1RJOMa/b9r8k81so5mtAN4D+ppZFbBS0oExlhlmlskdtuq6ZwcAPwL+mME8m4CH4/uX0mI+nM3Xvd6aNv3RdTERdjwGEZZNJfBpSb+SNNrMVjdR5xHA381svZmtZcvHuO0n6em4rE5hy2WVKE+aLgmXEC4P2Goc+VqqKukY2lP3je/vPGL65bVYalkHVvtf4IY2zD8LOKjuHzObAPw/YIc4SMCZMQENM7M9zKwuGda/FWL6M0hTaf+ngIL4/mrgGjMbSuhx6dbI/LVp89wInAZ8E/hLiz5d8A/gyPi+hi23+en1V5tZ3eGD9PoBGjqsIOCKtGWzl5n92cxeB4YTkucVki5qIjY1Me4m4IdxWV1SL9ZEedJ0Ha5yfOV64AdJx9GRRs61Dj9hpKP1XL+s/yEv/WodluqI+w1/CHwnw5N/GvMfoJuk76cNSz929hjw/dhViqS9JfVsQ32lwDvx/fgM5/kbocvzkBhPSx1BaPlCeOrQMEl5knYjdK0251lCtzCEFl+dx4BvSeoFIKmfpB0l7QJ8ZGa3Ea7PPojGPQN8XlK3WM64tHElwNK47E9pcO6EeNJ0iagcX/kocE/ScXSUnT/4uFuvSytZt3jAQTOueo+mu+Wy4ecTJo1d0JYCYsvqS8BRkhZKmkY42/SncZIbgdnAy5JeI7RqCxosLDMVwL2SngZWZBjjJkKL+h4zq82wnrpjmq8CvwS+E4c/S+jhqSQktEwenfa/wITYZV2aFtfjhEt8no9dqPcREt1QYFrs0j4fuKyJzzad0BJ+FXgAeBGoW28uJHSJ/wuYm0GcHUabW+TOdayhNw/didBFtm3SsbSnPd61N3/119q9ko6jI63sM7jy1f0n7EnbWmaNeRo4qqWtTEllwMPxBJVOIZ4A9DJwopm9UW/cOjPrlUxk2SGpl5mti2fHPgV818xy+jmo3tJ0iakcX/kuoesn0z3oTumoytSSpGPoaNt9MGfofrNunEc4MSab3gNObmW3bC1QqrSbG+QySUOAN4F/pydMxZsbAB15/Li9/DF+lpeB+3M9YYK3NF0OGHrz0LPpwmfUXnNdzbQdV2d0/KjLWbLTyGlz9/n6QUht6dasUwscPWHS2P9koSznWsVbmi5xleMrryIcS+pyZJbaYTVZu41ZZ7PLu1NH7DX/gWmYZeOG/Rd4wnRJ86TpcsX3CAf+u5RBbzNXaSdQbI12X/yfw8reeuRZ2tat9XfgV9mKybnW8qTpckLl+MqNhLuCLE06lmwaU5lannQMuWDPqsmj+73zZGsfYj0fGN/Gy0ucywpPmi5nVI6vXAJ8mS0v9O7Uhs3v3Gc3ZtM+b957VN9l06a0cLYVwOcmTBrb3pewOJcRT5oup1SOr3wB+G7ScWRDQa1t2uZDBicdRy7Zd87NY7Zb+dqUDCf/EBg3YdLYpG8I79zHPGm6nFM5vvIW4Kqk42ir/RfaXG15hxkHHFB5/ZjS1fOb66qtBk6YMGnstI6IyblMedJ0ueo8wl1GOq2jKm2rfuh2Uw6a8X+je61b/Ewjow349oRJYxt9QoZzSfGk6XJS5fjKWuBkOvGt9varsj5Jx5CrBDrkxYmjuq9f/nwDo38yYdLYWxsY7lziPGm6nFU5vrIG+Brh2YOdSlG1fdRrA4OSjiOXCcs/dNqlBxdvXDU9bfDPJkwae2ViQTnXDE+aLqfFFufX2fxMv07h4DdsjqAo6ThyXZ6lCke+cPHQwk1rZwDnTJg0dmLSMTnXFL+NnusUht48NB+YxOYnNuS0n91d++SBC+yopOPoJFIpFXxv3zmVNyYdiHPN8Zam6xQqx1fWVo6vPB24POlYMjFose3Q/FSOcE3uiZ4wXWfhSTMHSCqTtD796QuS1qWN+1qGZbzWzDQnSXpT0sMNjOsr6WFJr0qaLemfrfgc/5S0TQvn2UFStaTvZTJ95fjKC4CzaPhp8jmh53pb3W2TH8/MwBrgmMFz5zyQdCDOZcqTZu6Yb2bDGhheRjgZps3M7G4a7968FPiXmR1gZkOA8kzLVZBnZp8zs1UtDOtEYCrhTNmMVI6vvJqwTDa1sK4OMXKuzZP/tprzLnDU4LlzpiQdiHMt4T/s3DcRGB2fxH52bFE+Lenl+Dqs/gxx/LC0/5+VtH8z9ewMLK77x8xmps1/nqTpkmZKuiQOK5M0R9J1hGfh7SapStL2cfzXJU2Lcd8gKb+Rek8GzgF2ldQvs0UCleMr7wJGAW80N21HGz0rtSHpGHLcE8CBg+fO6RTPtXQunSfN3FcOPG1mw8zsKsJDeP/HzA4CTgL+0MA8NwKnAUjaGyhOT4KNuBb4s6T/Sjpf0i5x/qOBgcAIYBgwXNKRcZ59gFvM7EAze6uuIEmDY2yHx9ZzLXBK/Qol7QbsZGbTCNdjntT84tiscnzly8BBwG0tma+97bWUXZKOIUfVAD8Djh48d867SQfjXGtk48GwrmMVAtfElmQtsHcD09wLXCjpPOBbwE3NFWpmj0naEzgG+CwwQ9J+wNHxNSNO2ouQRBcBb5nZ1AaK+3/AcGC6JIDuhGRf31fZfPOCu4A/08KHUVeOr1wHnDr05qFPEBJ/z5bMn23brLPlRTXslWQMOaoK+NrguXMaupmBc52GJ83O52xgGXAAoafgE12BZvaRpH8BXwS+AhycScFm9j5wB3BHPFnoSEDAFWZ2Q/q0ksoIN9RuiICbzexnzVR5MtBXUl0rdBdJA82sxV2uleMrbx5689CphOTb0LHhDnHELHsD8DNnt3Q/8J3Bc+e09Hi3cznHu2dz31qgJO3/UmCpmaWAU4HGjhXeSOi6nR6TYZMkjZXUI74vAQYQWpOPAd+S1CuO6ydpx2aK+zdwQt10kraV1L9effsAPc2sn5mVmVkZcAWh9dkqleMr5wEjgWtaW0ZbHT47VZtU3TloA/D9wXPnnOAJ03UVnjRz30ygJl4KcjZwHTBe0lRC12yDrT0ze4lwSv9fM6xnOPCipJnA88CNZjbdzB4ntD6fl1RJuIl6SRPlYGazgQuAx2N5/yKcaJTuZOBv9YbdTwvOom1I5fjKjZXjK88kPJez2Z2FbOv/Hv2bn2qrMBsYMXjunElJB+JcNvkdgXJA7Op82Mz2y2KZuwBTgEGxVVo3fAxwrpkdm626ctXQm4fuBvwOOK4j6uv7vi2++obaXTuirhy2EbgauHjw3DkfJR2Mc9nmLc3cUAuUpt/coC0kfQN4ATi/XsI8idBS3SoeWVU5vvLtyvGVxwNHEJZHuzpyVqqqvevIcXcDgwfPnXOeJ0zXVXlL0201ht489CTCcdM92qP83/6p5tndVnB4e5Sd454Fzhk8d06775g4lzRvabqtRuX4yruBQcC5tENru9/Kre5SkzeBEwbPnXOEJ0y3tfCk6bYqleMrN1WOr/wtsBfheGdWbsXXf5ktyDP6ZqOsTmAl8CNgyOC5c+5POhjnOpJ3z7qt2tCbhw4ALgNOoA3XLZ/679qnPj/Njmx+yk5tDXAD8Eu/hMRtrTxpOgcMvXnoTsA3gdNpxTHPq6+vmdp3FSOzHlhumEZIlnf5CT5ua+dJ07k0Q28eKuB/gO8BXyCD1qfMUndNrF0t6NPe8XWgI/ifxQAABCJJREFUNYR7+v5x8Nw5ryYdjHO5wpOmc43ItPW5z9s29xe31XaV52d6q9K5JnjSdK4Zaa3P7xJuZt8jffx3/1n75KdftaOSiC1LlhNu8u+tSuea4UnTuRYYevPQYsKN7I+JryGTrq55cdt1md0UP0fUEm728AjwKPDS4LlzfEPgXAY8aTrXBkNvHrrbTb+tGdNjE2MIdx5q6FFtSaslPNrtGeBp4D9+9qtzreNJ07ksmjNo8I6E5DmScC1oGeF46DYdFMIyYGF8zSPcrWfq4Llz1nVQ/c51aZ40nesAcwYN3obNCbT+3340/oi3+jYSHtm2kPBg54Vp76sGz52zPmtBO+c+wZOmc845lyG/jZ5zzjmXIU+azm2lJJVJWp/+SDpJ50uaJWmmpFckHdrKsr8gqbyZaQbEOj5xvDXG9lor6z5N0jUNDP+npI46tuy6qFbfa9M51yXMN7NhAJJGAccCB5nZRknbA0WtKdTM/gH8o5lp5gPDGkqa7cHMPtcR9biuzVuazrk6OwMrzGwjgJmtMLMlAJKqJP1K0rT42isO/7ykFyTNkPSEpL5x+MetPUk3SfqDpOckLZB0QmsDlHS6pOmSXpV0v6Qezc/18bxVcUcASRdKmivpX5LulHRuW8t3WwdPms65Oo8Du0l6XdJ1kurf5WiNmY0AriE8Vg3CtZ8jzexA4C7gJ42UvTPhUpxjgYltiPEBMzvEzA4A5gDfbmkBkg4GjgcOBI6DLW5M0ebyXdfmSdM5B4CZrQOGE24XuBy4W9JpaZPcmfZ3VHy/K/CYpErgPGDfRop/0MxSZjYb2vTc0f0kPR3rO6WJ+ppyBPB3M1tvZmuBh7JcvuvCPGk65z5mZrVmNsXMLgZ+SGiRfTy6gfdXA9eY2VDCk2G6NVL0xrT3akOINwE/jPVd0kR9TWmq/myU77owT5rOOQAk7SNpYNqgYcBbaf+flPb3+fi+FHgnvh/fvhECUAIslVRIaAm2xjPA5yV1k9QLGJfl8l0X5mfPOufq9AKujpdl1ABvErpq6xRL+v/t3TFKxFAUhtH/9YJbsLOxdCG2dlb2LkBwHW5BbMQVWCmKIG5AQTsbwVauxUSZQvQiOiBzTpdJeEn3JWG4ucjsZnt7+u0gydEY4zHJeX7wAe8vrI8xHua295LsZzZs/j7JbWaR+8zOGGNrbvvjA+FVdTnGOElyM61zleR52t1dnyVlIhAsqTHGWpLTqtpoHHuXZLOqnv7gOl6qauW31/3mnCtV9TL9O/YsyW5VXS/yGvifvJ6F5fWaZHV+uMEivQ83yGzI/KIdTue+TnIsmHR50gSAJk+aANAkmgDQJJoA0CSaANAkmgDQJJoA0CSaANAkmgDQJJoA0CSaANAkmgDQJJoA0CSaANAkmgDQJJoA0CSaAND0Bv204eHuoneBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum League value:  1014\n",
      "Maximum League value:  [World] Friendly International\n",
      "The most frequent range of values in the League column is: (20, 30]\n",
      "Most frequent value of League column: 25.07\n",
      "Number of occurrences of most frequent value in League column: 8\n"
     ]
    }
   ],
   "source": [
    "column_name = 'League'\n",
    "\n",
    "# plt.hist(df[column_name], 100, edgecolor=\"black\")\n",
    "\n",
    "# plt.xlabel(column_name + \" ratings\")\n",
    "# plt.ylabel(\"Number of teams\")\n",
    "# plt.title(\"Distribution of \" + column_name + \" Ratings\")\n",
    "\n",
    "value_min = df[column_name].min()\n",
    "value_max = df[column_name].max()\n",
    "\n",
    "df_filtered = df[df['Overall'] >= 80]\n",
    "\n",
    "# Count the number of occurrences of each bin\n",
    "value_counts = df_filtered['League'].value_counts()\n",
    "\n",
    "# Vẽ biểu đồ tròn\n",
    "plt.pie(value_counts.values,autopct='',labels = value_counts.index)\n",
    "plt.title(\"Distribution of \" + column_name + \" ratings of Teams has Overall ratings equal than 80.\")\n",
    "plt.show()\n",
    "\n",
    "# # Find the bin with the highest count\n",
    "# most_frequent_bin = bin_counts.idxmax()\n",
    "\n",
    "# most_frequent_value = df[column_name].mode()[0]\n",
    "\n",
    "# num_occurrences = (df[column_name] == most_frequent_value).sum()\n",
    "\n",
    "# plt.yticks(range(0, num_occurrences + 10, 10))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "print(\"Minimum \" + column_name + \" value: \", value_min)\n",
    "print(\"Maximum \" + column_name + \" value: \", value_max)\n",
    "print(\"The most frequent range of values in the \" + column_name + \" column is:\", most_frequent_bin)\n",
    "print(\"Most frequent value of \" + column_name + \" column:\", most_frequent_value)\n",
    "print(\"Number of occurrences of most frequent value in \" + column_name + \" column:\", num_occurrences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSf3hA0O1OwE"
   },
   "source": [
    "* Nhận xét: Trong 100 đội bóng thì có 11 đội bóng tham gia giải Spain Laliga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaZWQTBeLNMP"
   },
   "source": [
    "## C. Đặt các câu hỏi có ý nghĩa cần trả lời\n",
    "\n",
    "### 5 câu hỏi mà nhóm trả lời bằng dữ liệu nêu trên:\n",
    "\n",
    "**Câu hỏi 1:** Các đội bóng nào có trung bình tuổi cao nhất và thấp nhất?\n",
    ">Ý nghĩa: câu hỏi này sẽ giúp phân tích đội hình của các đội bóng có tuổi trung bình cao và thấp, từ đó đánh giá khả năng cạnh tranh, phong độ, kinh nghiệm của đội bóng đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "id": "c00y7HTFRuCf",
    "outputId": "ce02db11-0a86-4396-de6a-fd2ddef747eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Các đội bóng nào có trung bình tuổi cao nhất</th>\n",
       "      <th>Whole team average age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>América Mineiro</td>\n",
       "      <td>29.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>São Paulo</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>China PR</td>\n",
       "      <td>28.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Chengdu Qianbao</td>\n",
       "      <td>28.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Shandong Luneng</td>\n",
       "      <td>28.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Northern Ireland</td>\n",
       "      <td>28.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Al Feiha</td>\n",
       "      <td>28.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Ulsan</td>\n",
       "      <td>28.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Sivasspor</td>\n",
       "      <td>28.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>César Vallejo</td>\n",
       "      <td>27.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Các đội bóng nào có trung bình tuổi cao nhất  Whole team average age\n",
       "337                              América Mineiro                   29.80\n",
       "99                                     São Paulo                   29.00\n",
       "71                                      China PR                   28.70\n",
       "444                              Chengdu Qianbao                   28.36\n",
       "442                              Shandong Luneng                   28.32\n",
       "391                             Northern Ireland                   28.22\n",
       "242                                     Al Feiha                   28.17\n",
       "347                                        Ulsan                   28.14\n",
       "109                                    Sivasspor                   28.07\n",
       "237                                César Vallejo                   27.90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Các đội bóng nào có trung bình tuổi thấp nhất</th>\n",
       "      <th>Whole team average age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>UCD</td>\n",
       "      <td>19.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Nordsjælland</td>\n",
       "      <td>21.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Shelbourne</td>\n",
       "      <td>21.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Genk</td>\n",
       "      <td>21.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>FC Groningen</td>\n",
       "      <td>21.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>New York RB</td>\n",
       "      <td>22.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>AZ</td>\n",
       "      <td>22.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Liverpool</td>\n",
       "      <td>22.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>Strømsgodset</td>\n",
       "      <td>22.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Viborg</td>\n",
       "      <td>22.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Các đội bóng nào có trung bình tuổi thấp nhất  Whole team average age\n",
       "414                                           UCD                   19.86\n",
       "266                                  Nordsjælland                   21.08\n",
       "408                                    Shelbourne                   21.72\n",
       "137                                          Genk                   21.75\n",
       "221                                  FC Groningen                   21.90\n",
       "85                                    New York RB                   22.03\n",
       "84                                             AZ                   22.10\n",
       "254                                     Liverpool                   22.23\n",
       "417                                  Strømsgodset                   22.28\n",
       "383                                        Viborg                   22.38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_10_max = df.nlargest(10, 'Whole team average age')[['Name', 'Whole team average age']]\n",
    "top_10_min = df.nsmallest(10, 'Whole team average age')[['Name', 'Whole team average age']]\n",
    "\n",
    "display(top_10_max.rename(columns={'Name': 'Các đội bóng nào có trung bình tuổi cao nhất'}))\n",
    "display(top_10_min.rename(columns={'Name': 'Các đội bóng nào có trung bình tuổi thấp nhất'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCVTYjRTXZzD"
   },
   "source": [
    "\n",
    "**Câu hỏi 2:** Đội bóng nào có Overall cao nhất trong danh sách?\n",
    ">Ý nghĩa: Câu hỏi này giúp tìm ra đội bóng có Overall cao nhất trong danh sách, từ đó đánh giá mức độ mạnh yếu của các đội bóng trong danh sách."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "bvMfs27-XbsH",
    "outputId": "0a33382f-582e-4ac3-9ce8-620ff074e19b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Đội bóng có Overall cao nhất trong danh sách</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manchester City</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Đội bóng có Overall cao nhất trong danh sách  Overall\n",
       "0                              Manchester City       85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_1_max = df.nlargest(1, ['Overall'])[['Name', 'Overall']]\n",
    "display(top_1_max.rename(columns={'Name': 'Đội bóng có Overall cao nhất trong danh sách'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKKZ04g7Yd25"
   },
   "source": [
    "\n",
    "**Câu hỏi 3:** Đội bóng nào có Attack cao nhất trong danh sách?\n",
    "> Ý nghĩa: Câu hỏi này giúp tìm ra đội bóng có chỉ số tấn công (Attack) cao nhất trong danh sách, từ đó đánh giá sức mạnh tấn công của các đội bóng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "Qs6qDXzwaF-J",
    "outputId": "701f8827-5b70-4fd3-b187-27bc23cd904b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Đội bóng có Attack cao nhất trong danh sách</th>\n",
       "      <th>Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Al Nassr</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Đội bóng có Attack cao nhất trong danh sách  Attack\n",
       "97                                    Al Nassr      87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_1_max = df.nlargest(1, ['Attack'])[['Name','Attack']]\n",
    "display(top_1_max.rename(columns={'Name': 'Đội bóng có Attack cao nhất trong danh sách'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH7rSIKIYfMP"
   },
   "source": [
    "\n",
    "**Câu hỏi 4:** Có bao nhiêu đội bóng đến từ mỗi quốc gia?\n",
    ">Ý nghĩa: Câu hỏi này giúp chúng ta có thể biết được sự phân bố của các đội bóng theo từng quốc gia. Điều này có thể giúp ta phân tích sức mạnh của bóng đá từng quốc gia, cũng như đánh giá chất lượng của từng giải đấu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "Cln2tAzdbKgL",
    "outputId": "d845b50d-341c-46e9-aa15-4b9515d91146"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Country'] = df['League'].str.split(']').str[0].str.strip('[')\n",
    "country_counts = df['Country'].value_counts()\n",
    "\n",
    "# print(df['Country'].value_counts())\n",
    "plt.pie(country_counts, labels=country_counts.index, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD_CN6LFYgSX"
   },
   "source": [
    "\n",
    "**Câu hỏi 5:** Số lượng đội bóng theo Uy tín quốc tế (International prestige)? \n",
    "\n",
    "> Ý nghĩa: Câu hỏi này giúp chúng ta có cái nhìn tổng quan về sự phân bố đội bóng theo mức độ uy tín quốc tế trên trang web sofifa.com. Số lượng đội bóng theo mức độ uy tín sẽ phản ánh mức độ quan tâm của cộng đồng đối với các giải đấu bóng đá trên toàn thế giới và cũng giúp chúng ta đánh giá được mức độ cạnh tranh của các giải đấu này.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "cSf3blctdw5G",
    "outputId": "2a176242-0169-44c6-cdaf-42588c42bf83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International prestige\n",
      "1     190\n",
      "2      54\n",
      "3      57\n",
      "4      40\n",
      "5      39\n",
      "6      22\n",
      "7      22\n",
      "8      12\n",
      "9       8\n",
      "10      7\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prestige_counts = df.groupby('International prestige').size()\n",
    "print(prestige_counts)\n",
    "plt.pie(prestige_counts.values, labels=prestige_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Number of Teams by International Prestige')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfxwtdW9YhOi"
   },
   "source": [
    "\n",
    "**Nguồn cảm hứng của câu hỏi?**\n",
    "\n",
    "> Tham khảo các thông tin liên quan đến đội bóng từ tin tức bóng đá trên các trang web chuyên về thể thao, các trận đấu bóng đá gần đây, thống kê về các cầu thủ và đội bóng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVc-10q_MonW"
   },
   "source": [
    "# PHẦN 2: MÔ HÌNH HÓA DỮ LIỆU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UcDklZcvf59"
   },
   "source": [
    "### A. Mô hình hóa dữ liệu và đánh giá mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CH5vVL11OwF"
   },
   "source": [
    "### THUẬT TOÁN PHÂN LOẠI RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly7LRBxnvf59"
   },
   "source": [
    "##### Phân loại đội bóng (uy tín, không uy tín) dựa trên các đặc trưng của đội bóng như (Overall, International prestige, Domestic prestige) sử dụng thuật toán phân loại Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRFp-icTvf59"
   },
   "source": [
    "Để giải quyết bài toán này, chúng ta sẽ thực hiện các bước sau:\n",
    "\n",
    "1. Khám phá và tiền xử lý dữ liệu.\n",
    "2. Tạo tập huấn luyện và tập kiểm tra.\n",
    "3. Huấn luyện mô hình phân loại Random Forest.\n",
    "4. Đánh giá mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbZrxUQ5vf59"
   },
   "source": [
    "**Bước 1: Khám phá và tiền xử lý dữ liệu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z45x-ugxn3P"
   },
   "source": [
    "* Kiểm tra các cột thiếu dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E156u0vHxb6I",
    "outputId": "2b0660e8-8f28-4aa5-f38d-1ab683b7f3dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                        0\n",
      "League                      0\n",
      "Overall                     0\n",
      "Attack                      0\n",
      "Midfield                    0\n",
      "Defence                     0\n",
      "Home stadium               15\n",
      "Rival team                  0\n",
      "International prestige      0\n",
      "Domestic prestige          31\n",
      "Club worth                 31\n",
      "Starting XI average age     0\n",
      "Whole team average age      0\n",
      "Captain                     0\n",
      "Short free kick             0\n",
      "Long free kick              0\n",
      "Left short free kick        0\n",
      "Right short free kick       0\n",
      "Penalties                   2\n",
      "Left corner                 0\n",
      "Right corner                0\n",
      "Country                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "306A_ekyx8xL"
   },
   "source": [
    "* Xóa các dòng dữ liệu NaN ở cột Domestic prestige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "TPFL8suIyHWp"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['Domestic prestige'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66uL6SJQvf59"
   },
   "source": [
    "**Bước 2: Tạo tập huấn luyện và tập kiểm tra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "pc31d7zqvf59"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw2N9ATevf59"
   },
   "source": [
    "* Chọn các đặc trưng để sử dụng cho mô hình phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "UqTgfC33vf59"
   },
   "outputs": [],
   "source": [
    "features = ['Overall', 'International prestige', 'Domestic prestige']\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAo_VR-3vf59"
   },
   "source": [
    "* Tạo nhãn dựa trên điều kiện uy tín (ví dụ: Overall >= 75 và (International prestige >= 6 hoặc Domestic prestige>= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "I4pRyPAyvf5-"
   },
   "outputs": [],
   "source": [
    "y = (df['Overall'] >= 70) & ((df['International prestige'] >= 5) | (df['Domestic prestige'] >= 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RiYbIHovf5-"
   },
   "source": [
    "* Chia dữ liệu thành tập huấn luyện và tập kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "QlTSoXARvf5-"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGyvfMf_vf5-"
   },
   "source": [
    "**Bước 3: Huấn luyện mô hình phân loại Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1BludnMvf5-"
   },
   "source": [
    "* Tạo mô hình Random Forest với 100 cây quyết định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "5pJRxkOOvf5-"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JRf7EUjvf5-"
   },
   "source": [
    "* Huấn luyện mô hình với dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "siT0tUuOvf5-",
    "outputId": "e9c2881f-3ecb-4188-cf32-8d453c1c3e6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTgqcWLFvf5-"
   },
   "source": [
    "**Bước 4: Đánh giá mô hình**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKSvxAA5vf5-"
   },
   "source": [
    "* Dự đoán nhãn cho tập kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "KkbS0gpavf5_"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUNIdSYuvf5_"
   },
   "source": [
    "* Tính độ chính xác của mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAyKUO8Yvf5_",
    "outputId": "8ad22713-721b-48e8-8439-037499b985ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Độ chính xác:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCOvTtaBvf5_"
   },
   "source": [
    "* Tính bảng phân loại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-u9oRmMCvf5_",
    "outputId": "de284e25-106d-4c40-c6f7-1dc61facc1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bảng phân loại:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00        55\n",
      "        True       1.00      1.00      1.00        29\n",
      "\n",
      "    accuracy                           1.00        84\n",
      "   macro avg       1.00      1.00      1.00        84\n",
      "weighted avg       1.00      1.00      1.00        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Bảng phân loại:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kundb_zvf5_"
   },
   "source": [
    "* Tính ma trận nhầm lẫn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trvg-Mzgvf5_",
    "outputId": "eb0165fa-4f60-4bb1-d0aa-59ab1ad72d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ma trận nhầm lẫn:\n",
      " [[55  0]\n",
      " [ 0 29]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Ma trận nhầm lẫn:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUCsgVLTvf5_"
   },
   "source": [
    "Sau khi thực hiện các bước trên, bạn sẽ có được kết quả đánh giá mô hình phân loại đội bóng (uy tín, không uy tín) sử dụng thuật toán Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-4GYAka1OwJ"
   },
   "source": [
    "### THUẬT TOÁN HỒI QUY TUYẾN TÍNH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "Ov5z3XNF1OwJ",
    "outputId": "7f303a96-e90f-447a-d3cf-da8b4a085cc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X=X_train,y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBZ0WyfR1OwJ",
    "outputId": "ee9b8889-8606-4254-8b38-96715250b34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác: 0.5640996660833686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = model.predict(X=X_test)\n",
    "accuracy = r2_score(y_test,y_pred)\n",
    "print(\"Độ chính xác:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXlegnnT1OwJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6CYctmP1OwJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZiyu_nL1OwK"
   },
   "source": [
    "### Nhận xét:\n",
    "Độ chính xác của mô hình hồi quy tuyến tính thấp hơn so với mô hình random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxdJNJFPl2GG"
   },
   "source": [
    "### PHẦN 3: TỔNG HỢP KẾT QUẢ\n",
    "\n",
    "\n",
    "Báo cáo đánh giá đồ án của Nhóm 7:\n",
    "\n",
    "Những khó khăn đã gặp phải:\n",
    "- Trong quá trình thực hiện đồ án, gặp khó khăn khi phân tích dữ liệu. Tuy nhiên, thông qua việc tra cứu tài liệu và học hỏi từ thành viên trong nhóm, vượt qua được khó khăn này.\n",
    "- Trong quá trình thực hiện đồ án, gặp khó khăn khi trình bày kết quả bằng biểu đồ. Tuy nhiên, sau khi học hỏi từ đồng đội và tìm hiểu thêm về cách sử dụng thư viện trực quan hóa dữ liệu, khắc phục được khó khăn này.\n",
    "\n",
    "Những kiến thức học hỏi được:\n",
    "\n",
    "- Học được cách thu thập và phân tích dữ liệu bằng ngôn ngữ Python. Đồng thời, học được cách sử dụng các thư viện hỗ trợ trong việc xử lý dữ liệu như Pandas và Numpy.\n",
    "- Học được cách trình bày kết quả bằng các biểu đồ trực quan. Đồng thời, nâng cao được kỹ năng lập trình bằng ngôn ngữ Python thông qua việc thực hiện đồ án.\n",
    "Nhóm của bạn:\n",
    "\n",
    "Nếu có nhiều thời gian hơn, sẽ cố gắng thực hiện một số công việc như sau:\n",
    "\n",
    "- Tìm hiểu thêm về các thư viện hỗ trợ trong việc xử lý dữ liệu để có thể áp dụng vào việc phân tích dữ liệu phức tạp hơn.\n",
    "- Tìm hiểu và áp dụng các phương pháp phân tích dữ liệu tiên tiến để có thể đưa ra kết luận chính xác và hữu ích hơn.\n",
    "Nâng cao kỹ năng lập trình bằng việc thực hiện các bài tập và dự án thực tế khác để có thể áp dụng kiến thức vào các dự án thực tế trong tương lai."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
